{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9MxlDzV0vD9r",
    "outputId": "b7b8ffba-947a-4759-e636-144840e9e2c8"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CODE POUR ACCÃ‰DER AU DOSSIER PARTAGÃ‰\n",
    "# ============================================================================\n",
    "\n",
    "from google.colab import drive\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# 1. Montez VOTRE Drive personnel\n",
    "print(\"ðŸ” Montez votre Drive personnel...\")\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 2. CrÃ©ez un raccourci (optionnel mais pratique)\n",
    "print(\"\\nðŸ“ Recherche du dossier partagÃ© 'ISIC_2019_Project'...\")\n",
    "\n",
    "# MÃ©thode 1: Chercher automatiquement\n",
    "!find '/content/drive' -name \"ISIC_2019_Project\" -type d 2>/dev/null\n",
    "\n",
    "# MÃ©thode 2: Ajouter un raccourci manuellement\n",
    "print(\"\"\"\n",
    "ðŸ’¡ Si le dossier n'est pas trouvÃ© automatiquement:\n",
    "\n",
    "1. Allez sur drive.google.com\n",
    "2. Dans \"PartagÃ©s avec moi\", trouvez \"ISIC_2019_Project\"\n",
    "3. Clic droit â†’ \"Ajouter un raccourci vers Drive\"\n",
    "4. Choisissez \"Mon Drive\"\n",
    "5. Le dossier apparaÃ®tra dans votre Drive principal\n",
    "\"\"\")\n",
    "\n",
    "# 3. Attendez que l'utilisateur confirme\n",
    "input(\"\\nâ³ Avez-vous ajoutÃ© le raccourci ? (Appuyez sur EntrÃ©e quand c'est fait)\")\n",
    "\n",
    "# 4. Configurer les chemins\n",
    "DRIVE_ROOT = Path('/content/drive/MyDrive/ISIC_2019_Project')\n",
    "\n",
    "if not DRIVE_ROOT.exists():\n",
    "    print(f\"\\nâŒ Dossier non trouvÃ© Ã : {DRIVE_ROOT}\")\n",
    "    print(\"\\nðŸ” Autres emplacements possibles...\")\n",
    "\n",
    "    # Chercher dans tout le Drive\n",
    "    import subprocess\n",
    "    result = subprocess.run(['find', '/content/drive', '-name', '*ISIC*', '-type', 'd'],\n",
    "                          capture_output=True, text=True)\n",
    "    paths = result.stdout.strip().split('\\n')\n",
    "\n",
    "    if paths and paths[0]:\n",
    "        print(\"\\nðŸ“‚ Dossiers trouvÃ©s:\")\n",
    "        for i, path in enumerate(paths[:5], 1):\n",
    "            print(f\"  {i}. {path}\")\n",
    "\n",
    "        # Prendre le premier\n",
    "        DRIVE_ROOT = Path(paths[0])\n",
    "        print(f\"\\nâœ… Utilisation du chemin: {DRIVE_ROOT}\")\n",
    "    else:\n",
    "        print(\"\\nâš ï¸  Aucun dossier ISIC trouvÃ©.\")\n",
    "        print(\"Veuillez entrer le chemin manuellement:\")\n",
    "        custom_path = input(\"Chemin: \")\n",
    "        DRIVE_ROOT = Path(custom_path)\n",
    "\n",
    "# 5. VÃ©rifier la structure\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“‹ VÃ©rification de la structure...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "DATA_DIR = DRIVE_ROOT / 'data'\n",
    "IMG_DIR = DATA_DIR / 'ISIC_2019_Training_Input' / 'ISIC_2019_Training_Input'\n",
    "GT_PATH = DATA_DIR / 'ISIC_2019_Training_GroundTruth.csv'\n",
    "META_PATH = DATA_DIR / 'ISIC_2019_Training_Metadata.csv'\n",
    "\n",
    "print(f\"ðŸ“ Racine: {DRIVE_ROOT}\")\n",
    "print(f\"ðŸ“Š Data dir: {'âœ… Existe' if DATA_DIR.exists() else 'âŒ Manquant'}\")\n",
    "print(f\"ðŸ–¼ï¸ Images: {'âœ… Existe' if IMG_DIR.exists() else 'âŒ Manquant'}\")\n",
    "print(f\"ðŸ“„ Ground Truth: {'âœ… Existe' if GT_PATH.exists() else 'âŒ Manquant'}\")\n",
    "print(f\"ðŸ“‹ Metadata: {'âœ… Existe' if META_PATH.exists() else 'âŒ Manquant'}\")\n",
    "\n",
    "# 6. Lister le contenu pour vÃ©rifier\n",
    "if DRIVE_ROOT.exists():\n",
    "    print(f\"\\nðŸ“ Contenu de {DRIVE_ROOT}:\")\n",
    "    !ls -la \"{DRIVE_ROOT}\"\n",
    "\n",
    "    if DATA_DIR.exists():\n",
    "        print(f\"\\nðŸ“Š Contenu de data/:\")\n",
    "        !ls -la \"{DATA_DIR}\"\n",
    "\n",
    "print(\"\\nâœ… Configuration terminÃ©e!\")\n",
    "print(\"Vous pouvez maintenant exÃ©cuter votre code EDA.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eT5VAiqQye1b",
    "outputId": "f1f8bdf8-bd99-45cf-ab82-d26905e6d8d3"
   },
   "outputs": [],
   "source": [
    "# MONTER LE DRIVE\n",
    "from google.colab import drive\n",
    "\n",
    "print(\"ðŸ” Montez votre Drive personnel...\")\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"\\nâœ… Drive montÃ© avec succÃ¨s!\")\n",
    "print(\"ðŸ“ VÃ©rification du dossier ISIC_2019_Project...\")\n",
    "\n",
    "from pathlib import Path\n",
    "DRIVE_ROOT = Path('/content/drive/MyDrive/ISIC_2019_Project')\n",
    "\n",
    "if DRIVE_ROOT.exists():\n",
    "    print(f\"âœ… Dossier trouvÃ©: {DRIVE_ROOT}\")\n",
    "    print(\"Contenu:\")\n",
    "    !ls -la \"{DRIVE_ROOT}\"\n",
    "else:\n",
    "    print(\"âŒ Dossier non trouvÃ©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "tMfV8v8eynSA",
    "outputId": "c208cec7-df21-4124-957d-8f906add8faf"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ISIC 2019 - EDA COMPLÃˆTE\n",
    "# ============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "from scipy import stats\n",
    "import warnings\n",
    "import shutil\n",
    "from google.colab import files\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION - CHEMINS (dÃ©jÃ  vÃ©rifiÃ©s)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"  ISIC 2019 - EDA avec Stockage Local\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Vos chemins (dÃ©jÃ  vÃ©rifiÃ©s)\n",
    "DRIVE_ROOT = Path('/content/drive/MyDrive/ISIC_2019_Project')\n",
    "DATA_DIR = DRIVE_ROOT / 'data'\n",
    "IMG_DIR = DATA_DIR / 'ISIC_2019_Training_Input' / 'ISIC_2019_Training_Input'\n",
    "GT_PATH = DATA_DIR / 'ISIC_2019_Training_GroundTruth.csv'\n",
    "META_PATH = DATA_DIR / 'ISIC_2019_Training_Metadata.csv'\n",
    "\n",
    "print(f\"\\nâœ… DonnÃ©es lues depuis Drive: {DATA_DIR}\")\n",
    "\n",
    "# RÃ©sultats EDA dans STOCKAGE LOCAL Colab âœ…\n",
    "EDA_DIR = Path('/content/eda_results')  # âŒ PAS dans Drive!\n",
    "EDA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Structure\n",
    "for subdir in ['plots', 'statistics', 'samples', 'reports', 'correlations', 'quality_checks']:\n",
    "    (EDA_DIR / subdir).mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"âœ… RÃ©sultats sauvegardÃ©s localement: {EDA_DIR}\")\n",
    "print(f\"âœ… Ã€ la fin: TÃ©lÃ©chargement ZIP automatique\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Configuration des classes\n",
    "CLASS_INFO = {\n",
    "    'MEL': 'Melanoma', 'NV': 'Nevus', 'BCC': 'Basal cell carcinoma',\n",
    "    'AK': 'Actinic keratosis', 'BKL': 'Benign keratosis',\n",
    "    'DF': 'Dermatofibroma', 'VASC': 'Vascular lesion', 'SCC': 'Squamous cell carcinoma'\n",
    "}\n",
    "classes = list(CLASS_INFO.keys())\n",
    "\n",
    "# Configuration pour Ã©conomiser l'espace\n",
    "DPI_RESOLUTION = 150  # Au lieu de 300\n",
    "SAVE_SAMPLE_IMAGES = True  # Mettre False pour Ã©conomiser\n",
    "MAX_IMAGES_TO_ANALYZE = 2000  # RÃ©duire si besoin\n",
    "\n",
    "# ============================================================================\n",
    "# 1. CHARGEMENT DES DONNÃ‰ES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nðŸ“¥ Chargement des donnÃ©es...\")\n",
    "gt_df = pd.read_csv(GT_PATH)\n",
    "meta_df = pd.read_csv(META_PATH)\n",
    "df = pd.merge(gt_df, meta_df, on='image', how='inner')\n",
    "\n",
    "print(f\"âœ… {len(df):,} Ã©chantillons chargÃ©s\")\n",
    "\n",
    "# Sauvegarder info de base\n",
    "basic_info = {\n",
    "    'dataset_name': 'ISIC 2019',\n",
    "    'total_samples': int(len(df)),\n",
    "    'num_classes': len(classes),\n",
    "    'classes': classes,\n",
    "    'analysis_date': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "with open(EDA_DIR / 'statistics' / '00_dataset_info.json', 'w') as f:\n",
    "    json.dump(basic_info, f, indent=2)\n",
    "\n",
    "# ============================================================================\n",
    "# 2. DISTRIBUTION DES CLASSES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"  1. DISTRIBUTION DES CLASSES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "class_counts = gt_df[classes].sum().sort_values(ascending=False)\n",
    "\n",
    "for cls, count in class_counts.items():\n",
    "    pct = (count / len(gt_df)) * 100\n",
    "    print(f\"{cls:6s}: {count:6,} ({pct:5.2f}%)\")\n",
    "\n",
    "# Calculs\n",
    "max_class = class_counts.max()\n",
    "min_class = class_counts.min()\n",
    "imbalance_ratio = max_class / min_class\n",
    "class_weights = {cls: len(gt_df) / (len(classes) * count)\n",
    "                 for cls, count in class_counts.items()}\n",
    "\n",
    "print(f\"\\nâš ï¸  Ratio dÃ©sÃ©quilibre: {imbalance_ratio:.1f}:1\")\n",
    "\n",
    "# Sauvegarder\n",
    "class_stats = {\n",
    "    'counts': {cls: int(count) for cls, count in class_counts.items()},\n",
    "    'percentages': {cls: float(count / len(gt_df) * 100) for cls, count in class_counts.items()},\n",
    "    'imbalance_ratio': float(imbalance_ratio),\n",
    "    'recommended_weights': {cls: float(w) for cls, w in class_weights.items()}\n",
    "}\n",
    "\n",
    "with open(EDA_DIR / 'statistics' / '01_class_distribution.json', 'w') as f:\n",
    "    json.dump(class_stats, f, indent=2)\n",
    "\n",
    "pd.DataFrame(class_stats['counts'].items(), columns=['Class', 'Count']).to_csv(\n",
    "    EDA_DIR / 'statistics' / '01_class_distribution.csv', index=False\n",
    ")\n",
    "\n",
    "# Visualisation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "class_counts.plot(kind='bar', ax=axes[0], color='steelblue', edgecolor='black')\n",
    "axes[0].set_title('Distribution des Classes', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xlabel('Classe')\n",
    "axes[0].set_ylabel('Nombre')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "for i, (cls, count) in enumerate(class_counts.items()):\n",
    "    axes[0].text(i, count + 200, f'{count:,}', ha='center', fontweight='bold', fontsize=8)\n",
    "\n",
    "axes[1].pie(class_counts, labels=class_counts.index, autopct='%1.1f%%',\n",
    "           startangle=90, colors=plt.cm.Set3(range(len(classes))))\n",
    "axes[1].set_title('Proportion des Classes', fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(EDA_DIR / 'plots' / '01_class_distribution.png', dpi=DPI_RESOLUTION, bbox_inches='tight')\n",
    "print(f\"ðŸ’¾ SauvegardÃ©: 01_class_distribution.png\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# 3. MÃ‰TADONNÃ‰ES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"  2. MÃ‰TADONNÃ‰ES CLINIQUES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "metadata_stats = {}\n",
    "\n",
    "if 'age_approx' in meta_df.columns:\n",
    "    age_data = meta_df['age_approx'].dropna()\n",
    "    metadata_stats['age'] = {\n",
    "        'mean': float(age_data.mean()),\n",
    "        'median': float(age_data.median()),\n",
    "        'std': float(age_data.std()),\n",
    "        'min': float(age_data.min()),\n",
    "        'max': float(age_data.max()),\n",
    "        'missing_pct': float(meta_df['age_approx'].isna().sum() / len(meta_df) * 100)\n",
    "    }\n",
    "    print(f\"ðŸ‘¤ Ã‚ge: {metadata_stats['age']['mean']:.1f} Â± {metadata_stats['age']['std']:.1f} ans\")\n",
    "\n",
    "if 'sex' in meta_df.columns:\n",
    "    sex_counts = meta_df['sex'].value_counts()\n",
    "    metadata_stats['sex'] = {\n",
    "        'distribution': {str(k): int(v) for k, v in sex_counts.items()},\n",
    "        'missing_pct': float(meta_df['sex'].isna().sum() / len(meta_df) * 100)\n",
    "    }\n",
    "    print(f\"âš¥ Sexe: {dict(sex_counts)}\")\n",
    "\n",
    "if 'anatom_site_general' in meta_df.columns:\n",
    "    site_counts = meta_df['anatom_site_general'].value_counts()\n",
    "    metadata_stats['anatomical_site'] = {\n",
    "        'unique_sites': int(len(site_counts)),\n",
    "        'top_5': {str(k): int(v) for k, v in site_counts.head(5).items()}\n",
    "    }\n",
    "    print(f\"ðŸ“ Sites: {metadata_stats['anatomical_site']['unique_sites']} uniques\")\n",
    "\n",
    "with open(EDA_DIR / 'statistics' / '02_metadata_stats.json', 'w') as f:\n",
    "    json.dump(metadata_stats, f, indent=2)\n",
    "\n",
    "# Visualisation mÃ©tadonnÃ©es\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "if 'age_approx' in meta_df.columns:\n",
    "    age_clean = meta_df['age_approx'].dropna()\n",
    "    axes[0, 0].hist(age_clean, bins=40, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "    axes[0, 0].axvline(age_clean.mean(), color='red', linestyle='--', linewidth=2,\n",
    "                      label=f'Moyenne: {age_clean.mean():.1f}')\n",
    "    axes[0, 0].set_title('Distribution Ã‚ge', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Ã‚ge (annÃ©es)')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "if 'sex' in meta_df.columns:\n",
    "    sex_counts.plot(kind='bar', ax=axes[0, 1], color=['lightcoral', 'lightblue'],\n",
    "                   edgecolor='black')\n",
    "    axes[0, 1].set_title('Distribution Sexe', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=0)\n",
    "    axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "if 'anatom_site_general' in meta_df.columns:\n",
    "    site_counts.head(8).plot(kind='barh', ax=axes[1, 0], color='lightgreen',\n",
    "                            edgecolor='black')\n",
    "    axes[1, 0].set_title('Top 8 Sites Anatomiques', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "axes[1, 1].axis('off')\n",
    "axes[1, 1].text(0.5, 0.5, 'MÃ©tadonnÃ©es\\nAnalysÃ©es', ha='center', va='center',\n",
    "               fontsize=16, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(EDA_DIR / 'plots' / '02_metadata.png', dpi=DPI_RESOLUTION, bbox_inches='tight')\n",
    "print(f\"ðŸ’¾ SauvegardÃ©: 02_metadata.png\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# 4. PROPRIÃ‰TÃ‰S DES IMAGES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"  3. PROPRIÃ‰TÃ‰S DES IMAGES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "sample_images = gt_df.sample(n=min(MAX_IMAGES_TO_ANALYZE, len(gt_df)), random_state=42)\n",
    "\n",
    "widths, heights, aspect_ratios, file_sizes = [], [], [], []\n",
    "\n",
    "print(\"ðŸ“Š Analyse des propriÃ©tÃ©s d'images...\")\n",
    "for idx, row in tqdm(sample_images.iterrows(), total=len(sample_images), desc=\"Analyse\"):\n",
    "    img_path = IMG_DIR / f\"{row['image']}.jpg\"\n",
    "\n",
    "    if img_path.exists():\n",
    "        try:\n",
    "            file_sizes.append(img_path.stat().st_size / 1024)\n",
    "            img = Image.open(img_path)\n",
    "            w, h = img.size\n",
    "            widths.append(w)\n",
    "            heights.append(h)\n",
    "            aspect_ratios.append(w / h)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "image_stats = {\n",
    "    'sample_size': len(widths),\n",
    "    'width': {'mean': float(np.mean(widths)), 'std': float(np.std(widths)),\n",
    "              'min': int(np.min(widths)), 'max': int(np.max(widths))},\n",
    "    'height': {'mean': float(np.mean(heights)), 'std': float(np.std(heights)),\n",
    "               'min': int(np.min(heights)), 'max': int(np.max(heights))},\n",
    "    'aspect_ratio': {'mean': float(np.mean(aspect_ratios)), 'std': float(np.std(aspect_ratios))},\n",
    "    'file_size_kb': {'mean': float(np.mean(file_sizes)), 'std': float(np.std(file_sizes))}\n",
    "}\n",
    "\n",
    "print(f\"ðŸ“ {len(widths):,} images analysÃ©es:\")\n",
    "print(f\"   Dimensions: {image_stats['width']['mean']:.0f}Ã—{image_stats['height']['mean']:.0f} px\")\n",
    "print(f\"   Taille: {image_stats['file_size_kb']['mean']:.1f} KB\")\n",
    "\n",
    "with open(EDA_DIR / 'statistics' / '03_image_properties.json', 'w') as f:\n",
    "    json.dump(image_stats, f, indent=2)\n",
    "\n",
    "# Visualisation\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "axes[0, 0].hist(widths, bins=40, color='coral', edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].axvline(np.mean(widths), color='red', linestyle='--', linewidth=2)\n",
    "axes[0, 0].set_title('Distribution Largeurs', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Pixels')\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "axes[0, 1].hist(heights, bins=40, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].axvline(np.mean(heights), color='red', linestyle='--', linewidth=2)\n",
    "axes[0, 1].set_title('Distribution Hauteurs', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Pixels')\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "axes[1, 0].hist(aspect_ratios, bins=40, color='lightgreen', edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].axvline(1.0, color='purple', linestyle=':', linewidth=2, label='CarrÃ© (1:1)')\n",
    "axes[1, 0].set_title('Ratios d\\'Aspect', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "axes[1, 1].hist(file_sizes, bins=40, color='lightyellow', edgecolor='black', alpha=0.7)\n",
    "axes[1, 1].set_title('Tailles Fichiers', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('KB')\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(EDA_DIR / 'plots' / '03_image_properties.png', dpi=DPI_RESOLUTION, bbox_inches='tight')\n",
    "print(f\"ðŸ’¾ SauvegardÃ©: 03_image_properties.png\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# 5. Ã‰CHANTILLONS D'IMAGES (optionnel)\n",
    "# ============================================================================\n",
    "\n",
    "if SAVE_SAMPLE_IMAGES:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"  4. Ã‰CHANTILLONS D'IMAGES\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Grille complÃ¨te\n",
    "    fig, axes = plt.subplots(8, 5, figsize=(16, 24))\n",
    "    fig.suptitle('Ã‰chantillons par Classe', fontsize=14, fontweight='bold')\n",
    "\n",
    "    for i, cls in enumerate(classes):\n",
    "        samples = gt_df[gt_df[cls] == 1].sample(n=min(5, (gt_df[cls] == 1).sum()),\n",
    "                                                random_state=42)\n",
    "\n",
    "        for j, (idx, row) in enumerate(samples.iterrows()):\n",
    "            img_path = IMG_DIR / f\"{row['image']}.jpg\"\n",
    "\n",
    "            if img_path.exists():\n",
    "                try:\n",
    "                    img = Image.open(img_path)\n",
    "                    axes[i, j].imshow(img)\n",
    "                    axes[i, j].axis('off')\n",
    "                    if j == 0:\n",
    "                        axes[i, j].set_title(f\"{cls}\\n({class_counts[cls]:,})\",\n",
    "                                           fontsize=9, fontweight='bold', loc='left')\n",
    "                except:\n",
    "                    axes[i, j].axis('off')\n",
    "            else:\n",
    "                axes[i, j].axis('off')\n",
    "                axes[i, j].text(0.5, 0.5, 'Image\\nnon trouvÃ©e',\n",
    "                               ha='center', va='center', fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(EDA_DIR / 'plots' / '04_samples_grid.png', dpi=DPI_RESOLUTION, bbox_inches='tight')\n",
    "    print(f\"ðŸ’¾ SauvegardÃ©: 04_samples_grid.png\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # Sauvegarder 2 Ã©chantillons par classe (lÃ©ger)\n",
    "    print(\"\\nðŸ“¸ Sauvegarde Ã©chantillons individuels...\")\n",
    "    for cls in classes:\n",
    "        samples = gt_df[gt_df[cls] == 1].sample(n=min(2, (gt_df[cls] == 1).sum()),\n",
    "                                               random_state=42)\n",
    "        saved_count = 0\n",
    "        for idx, (_, row) in enumerate(samples.iterrows(), 1):\n",
    "            img_path = IMG_DIR / f\"{row['image']}.jpg\"\n",
    "            if img_path.exists():\n",
    "                try:\n",
    "                    img = Image.open(img_path)\n",
    "                    # RÃ©duire taille pour Ã©conomiser\n",
    "                    img.thumbnail((400, 400))\n",
    "                    img.save(EDA_DIR / 'samples' / f'{cls}_sample_{idx}.jpg', quality=85)\n",
    "                    saved_count += 1\n",
    "                except:\n",
    "                    continue\n",
    "        print(f\"   âœ… {cls}: {saved_count} Ã©chantillons sauvegardÃ©s\")\n",
    "\n",
    "# ============================================================================\n",
    "# 6. RAPPORT MARKDOWN\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"  5. GÃ‰NÃ‰RATION DU RAPPORT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "report = f\"\"\"# ISIC 2019 - Rapport EDA\n",
    "\n",
    "**Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "**Ã‰chantillons:** {len(df):,}\n",
    "**Classes:** {len(classes)}\n",
    "\n",
    "## Distribution des Classes\n",
    "\n",
    "| Classe | Count | % | Poids |\n",
    "|--------|-------|---|-------|\n",
    "\"\"\"\n",
    "\n",
    "for cls in classes:\n",
    "    count = class_counts[cls]\n",
    "    pct = count / len(gt_df) * 100\n",
    "    weight = class_weights[cls]\n",
    "    report += f\"| {cls} | {count:,} | {pct:.2f}% | {weight:.3f} |\\n\"\n",
    "\n",
    "report += f\"\"\"\n",
    "\n",
    "**Ratio dÃ©sÃ©quilibre:** {imbalance_ratio:.1f}:1\n",
    "\n",
    "## MÃ©tadonnÃ©es\n",
    "\n",
    "- **Ã‚ge moyen:** {metadata_stats.get('age', {}).get('mean', 0):.1f} ans\n",
    "- **Sites anatomiques:** {metadata_stats.get('anatomical_site', {}).get('unique_sites', 0)} uniques\n",
    "\n",
    "## Images\n",
    "\n",
    "- **Dimensions moyennes:** {image_stats['width']['mean']:.0f}Ã—{image_stats['height']['mean']:.0f} px\n",
    "- **Ratio d'aspect:** {image_stats['aspect_ratio']['mean']:.2f}\n",
    "- **Taille fichier:** {image_stats['file_size_kb']['mean']:.1f} KB\n",
    "\n",
    "## Recommandations\n",
    "\n",
    "1. âš ï¸ **Class weighting obligatoire** (ratio {imbalance_ratio:.0f}:1)\n",
    "2. ðŸ–¼ï¸ **Resize Ã  224Ã—224** pixels\n",
    "3. ðŸŽ¨ **Data augmentation** pour classes minoritaires\n",
    "4. ðŸ“Š **Metrics:** AUC-ROC, Balanced Accuracy\n",
    "\n",
    "---\n",
    "\n",
    "GÃ©nÃ©rÃ© automatiquement - PrÃªt pour GitHub\n",
    "\"\"\"\n",
    "\n",
    "with open(EDA_DIR / 'reports' / 'EDA_REPORT.md', 'w', encoding='utf-8') as f:\n",
    "    f.write(report)\n",
    "\n",
    "# README\n",
    "readme = f\"\"\"# EDA Results - ISIC 2019\n",
    "\n",
    "RÃ©sultats de l'analyse exploratoire complÃ¨te.\n",
    "\n",
    "## Contenu\n",
    "\n",
    "- `plots/` - Visualisations\n",
    "- `statistics/` - Stats JSON/CSV\n",
    "- `samples/` - Images Ã©chantillons\n",
    "- `reports/` - Rapports Markdown\n",
    "\n",
    "## Insights ClÃ©s\n",
    "\n",
    "- **DÃ©sÃ©quilibre:** {imbalance_ratio:.1f}:1 â†’ Class weighting nÃ©cessaire\n",
    "- **Images:** {image_stats['width']['mean']:.0f}Ã—{image_stats['height']['mean']:.0f} px moyenne\n",
    "- **Ã‚ge moyen:** {metadata_stats.get('age', {}).get('mean', 0):.1f} ans\n",
    "\n",
    "Voir `EDA_REPORT.md` pour dÃ©tails complets.\n",
    "\"\"\"\n",
    "\n",
    "with open(EDA_DIR / 'README.md', 'w', encoding='utf-8') as f:\n",
    "    f.write(readme)\n",
    "\n",
    "print(\" Rapports gÃ©nÃ©rÃ©s!\")\n",
    "\n",
    "# ============================================================================\n",
    "# 7. CRÃ‰ATION DU ZIP ET TÃ‰LÃ‰CHARGEMENT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"  6. CRÃ‰ATION DU ZIP\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "zip_path = '/content/eda_results_complete'\n",
    "shutil.make_archive(zip_path, 'zip', EDA_DIR)\n",
    "\n",
    "# Statistiques du ZIP\n",
    "zip_size_mb = Path(f'{zip_path}.zip').stat().st_size / (1024 * 1024)\n",
    "\n",
    "print(f\"\\nðŸ“¦ ZIP crÃ©Ã©: eda_results_complete.zip ({zip_size_mb:.1f} MB)\")\n",
    "\n",
    "# Compter les fichiers\n",
    "import os\n",
    "plot_count = len([f for f in os.listdir(EDA_DIR / 'plots') if f.endswith('.png')])\n",
    "stats_count = len([f for f in os.listdir(EDA_DIR / 'statistics')])\n",
    "samples_count = len([f for f in os.listdir(EDA_DIR / 'samples') if f.endswith('.jpg')])\n",
    "reports_count = len([f for f in os.listdir(EDA_DIR / 'reports')])\n",
    "\n",
    "print(f\"\\n Contenu:\")\n",
    "print(f\"   â€¢ {plot_count} visualisations\")\n",
    "print(f\"   â€¢ {stats_count} fichiers stats\")\n",
    "print(f\"   â€¢ {samples_count} Ã©chantillons\")\n",
    "print(f\"   â€¢ {reports_count} rapports\")\n",
    "\n",
    "print(\"\\n TÃ©lÃ©chargement automatique du ZIP...\")\n",
    "files.download(f'{zip_path}.zip')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"  EDA TERMINÃ‰E!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n Prochaines Ã©tapes:\")\n",
    "print(f\"   1. Le ZIP ({zip_size_mb:.1f} MB) a Ã©tÃ© tÃ©lÃ©chargÃ© automatiquement\")\n",
    "print(f\"   2. Extraire le ZIP localement sur votre machine\")\n",
    "print(f\"   3. Copier le dossier eda_results/ dans votre repo GitHub\")\n",
    "print(f\"   4. Commit & push vers GitHub\")\n",
    "print(f\"   5. Les donnÃ©es originales restent dans le Drive partagÃ©\")\n",
    "print(f\"\\n Taille optimisÃ©e pour GitHub\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zQv8Uak422lW",
    "outputId": "eaee949e-e15b-4f17-f022-0c838384445c"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"\\nðŸ” Recherche du bon chemin d'images...\")\n",
    "\n",
    "# Option 1: Essayer diffÃ©rents chemins\n",
    "possible_img_paths = [\n",
    "    DATA_DIR / 'ISIC_2019_Training_Input' / 'ISIC_2019_Training_Input',\n",
    "    DATA_DIR / 'ISIC_2019_Training_Input',\n",
    "    DATA_DIR / 'images',\n",
    "    DATA_DIR / 'train',\n",
    "    DRIVE_ROOT / 'images'\n",
    "]\n",
    "\n",
    "IMG_DIR = None\n",
    "for test_path in possible_img_paths:\n",
    "    if test_path.exists():\n",
    "        # VÃ©rifier qu'il y a des fichiers .jpg\n",
    "        jpg_files = list(test_path.glob(\"*.jpg\"))\n",
    "        if jpg_files:\n",
    "            IMG_DIR = test_path\n",
    "            print(f\"âœ… Images trouvÃ©es dans: {test_path}\")\n",
    "            print(f\"   {len(jpg_files):,} fichiers .jpg trouvÃ©s\")\n",
    "            break\n",
    "\n",
    "if IMG_DIR is None:\n",
    "    print(\"âŒ Aucun dossier d'images trouvÃ© avec des .jpg\")\n",
    "    print(\"\\nðŸ”Ž Recherche dans tout le dataset...\")\n",
    "    !find \"{DATA_DIR}\" -name \"*.jpg\" -type f 2>/dev/null | head -5\n",
    "\n",
    "    # Demander le chemin manuellement\n",
    "    print(\"\\nðŸŽ¯ Entrez le chemin COMPLET vers le dossier d'images:\")\n",
    "    custom_path = input(\"Chemin: \")\n",
    "    IMG_DIR = Path(custom_path)\n",
    "\n",
    "# VÃ©rification finale\n",
    "if IMG_DIR.exists():\n",
    "    image_count = len(list(IMG_DIR.glob(\"*.jpg\")))\n",
    "    print(f\"\\nðŸ“Š Dossier images confirmÃ©: {IMG_DIR}\")\n",
    "    print(f\"ðŸ“¸ Nombre d'images .jpg: {image_count:,}\")\n",
    "else:\n",
    "    print(f\"âŒ Chemin invalide: {IMG_DIR}\")\n",
    "    raise FileNotFoundError(\"Dossier d'images non trouvÃ©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q5BOcWqw39y1",
    "outputId": "c7f87415-fd88-4101-cdd0-f20f75bf23d4"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DIAGNOSTIC DES NOMS D'IMAGES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"ðŸ” Diagnostic des correspondances image/CSV...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Voir les premiers noms dans le CSV\n",
    "print(\"\\nðŸ“„ Noms dans Ground Truth CSV (premiers 5):\")\n",
    "print(gt_df['image'].head().tolist())\n",
    "\n",
    "# 2. Voir les premiers fichiers .jpg\n",
    "print(\"\\nðŸ–¼ï¸  Fichiers .jpg dans le dossier (premiers 5):\")\n",
    "jpg_files = list(IMG_DIR.glob(\"*.jpg\"))\n",
    "print([f.name for f in jpg_files[:5]])\n",
    "\n",
    "# 3. VÃ©rifier la correspondance\n",
    "print(\"\\nðŸ”— Test de correspondance...\")\n",
    "sample_image_name = gt_df['image'].iloc[0]\n",
    "expected_path = IMG_DIR / f\"{sample_image_name}.jpg\"\n",
    "print(f\"Premier nom CSV: {sample_image_name}\")\n",
    "print(f\"Chemin attendu: {expected_path}\")\n",
    "print(f\"Existe? {expected_path.exists()}\")\n",
    "\n",
    "# 4. VÃ©rifier les extensions\n",
    "print(\"\\nðŸ“‹ Analyse des extensions dans le CSV:\")\n",
    "# Regarder s'il y a dÃ©jÃ  .jpg dans les noms\n",
    "has_extension = gt_df['image'].str.endswith('.jpg').any()\n",
    "print(f\"Les noms contiennent dÃ©jÃ  .jpg? {has_extension}\")\n",
    "\n",
    "if has_extension:\n",
    "    print(\"âš ï¸  Les noms dans le CSV ont dÃ©jÃ  l'extension .jpg\")\n",
    "    # Compter combien\n",
    "    with_extension = gt_df['image'].str.endswith('.jpg').sum()\n",
    "    print(f\"   {with_extension}/{len(gt_df)} noms avec .jpg\")\n",
    "\n",
    "    # VÃ©rifier un nom spÃ©cifique\n",
    "    sample_with_ext = gt_df[gt_df['image'].str.endswith('.jpg')]['image'].iloc[0] if with_extension > 0 else None\n",
    "    if sample_with_ext:\n",
    "        test_path = IMG_DIR / sample_with_ext\n",
    "        print(f\"   Exemple: {sample_with_ext} â†’ {test_path.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c_IrKsD64PJp",
    "outputId": "d01b1bbf-f3be-4926-d299-f7b6aa791892"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ANALYSE DES IMAGES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"ðŸ” Analyse corrigÃ©e de la couverture des images...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Analyser les formats de noms dans le CSV\n",
    "csv_ids = gt_df['image'].tolist()\n",
    "print(f\"\\nðŸ“„ Formats des noms dans le CSV (10 premiers):\")\n",
    "for i, name in enumerate(csv_ids[:10]):\n",
    "    print(f\"  {i+1:2}. {name}\")\n",
    "\n",
    "# 2. Extraire les IDs numÃ©riques (gÃ©rer les noms non standard)\n",
    "csv_ids_numeric = []\n",
    "non_numeric_ids = []\n",
    "\n",
    "for id_str in csv_ids:\n",
    "    # Essayer d'extraire les chiffres\n",
    "    import re\n",
    "    match = re.search(r'(\\d+)', id_str)\n",
    "    if match:\n",
    "        csv_ids_numeric.append(int(match.group(1)))\n",
    "    else:\n",
    "        non_numeric_ids.append(id_str)\n",
    "\n",
    "print(f\"\\nðŸ“Š Analyse des IDs:\")\n",
    "print(f\"   IDs numÃ©riques extraits: {len(csv_ids_numeric):,}\")\n",
    "print(f\"   IDs non numÃ©riques: {len(non_numeric_ids):,}\")\n",
    "if non_numeric_ids:\n",
    "    print(f\"   Exemples d'IDs non numÃ©riques: {non_numeric_ids[:5]}\")\n",
    "\n",
    "# 3. IDs dans les fichiers JPG\n",
    "jpg_files = list(IMG_DIR.glob(\"*.jpg\"))\n",
    "print(f\"\\nðŸ–¼ï¸  Fichiers .jpg trouvÃ©s: {len(jpg_files):,}\")\n",
    "\n",
    "# Analyser les noms de fichiers\n",
    "jpg_ids_numeric = []\n",
    "jpg_non_numeric = []\n",
    "\n",
    "for f in jpg_files:\n",
    "    stem = f.stem\n",
    "    match = re.search(r'(\\d+)', stem)\n",
    "    if match:\n",
    "        jpg_ids_numeric.append(int(match.group(1)))\n",
    "    else:\n",
    "        jpg_non_numeric.append(stem)\n",
    "\n",
    "print(f\"\\nðŸ“Š Analyse des fichiers:\")\n",
    "print(f\"   IDs numÃ©riques: {len(jpg_ids_numeric):,}\")\n",
    "print(f\"   IDs non numÃ©riques: {len(jpg_non_numeric):,}\")\n",
    "\n",
    "# 4. Statistiques si nous avons des IDs numÃ©riques\n",
    "if csv_ids_numeric and jpg_ids_numeric:\n",
    "    min_csv_id = min(csv_ids_numeric)\n",
    "    max_csv_id = max(csv_ids_numeric)\n",
    "    min_jpg_id = min(jpg_ids_numeric)\n",
    "    max_jpg_id = max(jpg_ids_numeric)\n",
    "\n",
    "    print(f\"\\nðŸ“ˆ Plages d'IDs:\")\n",
    "    print(f\"   CSV: ISIC_{min_csv_id:07d} Ã  ISIC_{max_csv_id:07d}\")\n",
    "    print(f\"   JPG: ISIC_{min_jpg_id:07d} Ã  ISIC_{max_jpg_id:07d}\")\n",
    "\n",
    "    # Calculer le chevauchement\n",
    "    csv_set = set(csv_ids_numeric)\n",
    "    jpg_set = set(jpg_ids_numeric)\n",
    "    overlap = csv_set.intersection(jpg_set)\n",
    "\n",
    "    print(f\"\\nðŸ”— Chevauchement:\")\n",
    "    print(f\"   IDs communs: {len(overlap):,}\")\n",
    "    print(f\"   Coverage: {len(overlap)/len(csv_set)*100:.1f}%\")\n",
    "\n",
    "    # Images manquantes\n",
    "    missing = csv_set - jpg_set\n",
    "    if missing:\n",
    "        print(f\"\\nâŒ IDs manquants dans les fichiers JPG:\")\n",
    "        print(f\"   Nombre: {len(missing):,}\")\n",
    "        print(f\"   Exemples: {sorted(list(missing))[:5]}\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  Impossible d'analyser les plages (IDs non numÃ©riques)\")\n",
    "\n",
    "# 5. VÃ‰RIFICATION DIRECTE DES CORRESPONDANCES\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… VÃ‰RIFICATION DIRECTE DES CORRESPONDANCES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Fonction amÃ©liorÃ©e pour trouver les images\n",
    "def find_image_path_v2(image_name):\n",
    "    \"\"\"\n",
    "    Trouve le chemin d'une image avec plusieurs stratÃ©gies\n",
    "    \"\"\"\n",
    "    # StratÃ©gie 1: Nom exact + .jpg\n",
    "    path1 = IMG_DIR / f\"{image_name}.jpg\"\n",
    "    if path1.exists():\n",
    "        return path1\n",
    "\n",
    "    # StratÃ©gie 2: Nom exact (dÃ©jÃ  avec .jpg)\n",
    "    if image_name.endswith('.jpg'):\n",
    "        path2 = IMG_DIR / image_name\n",
    "        if path2.exists():\n",
    "            return path2\n",
    "\n",
    "    # StratÃ©gie 3: Extraire l'ID numÃ©rique et chercher\n",
    "    match = re.search(r'(\\d+)', image_name)\n",
    "    if match:\n",
    "        num_id = match.group(1)\n",
    "        # Chercher tous les fichiers contenant cet ID\n",
    "        for f in IMG_DIR.glob(f\"*{num_id}*.jpg\"):\n",
    "            return f\n",
    "\n",
    "    # StratÃ©gie 4: Chercher par pattern\n",
    "    patterns = [\n",
    "        f\"*{image_name}*.jpg\",\n",
    "        f\"*{image_name.replace('_', '*')}*.jpg\",\n",
    "    ]\n",
    "\n",
    "    for pattern in patterns:\n",
    "        matches = list(IMG_DIR.glob(pattern))\n",
    "        if matches:\n",
    "            return matches[0]\n",
    "\n",
    "    return None\n",
    "\n",
    "# Tester avec quelques exemples\n",
    "print(\"\\nðŸ§ª Test de correspondance (10 premiers):\")\n",
    "test_samples = gt_df['image'].head(10).tolist()\n",
    "\n",
    "found_count = 0\n",
    "for i, img_name in enumerate(test_samples):\n",
    "    path = find_image_path_v2(img_name)\n",
    "    status = \"âœ…\" if path and path.exists() else \"âŒ\"\n",
    "    print(f\"  {i+1:2}. {img_name:30} â†’ {status}\")\n",
    "    if path and path.exists():\n",
    "        found_count += 1\n",
    "\n",
    "print(f\"\\nðŸ“Š Taux de succÃ¨s: {found_count}/{len(test_samples)} ({found_count/len(test_samples)*100:.0f}%)\")\n",
    "\n",
    "# 6. ANALYSE COMPLÃˆTE DE LA DISPONIBILITÃ‰\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“Š ANALYSE COMPLÃˆTE DE DISPONIBILITÃ‰\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Analyser un Ã©chantillon pour aller plus vite\n",
    "SAMPLE_SIZE = min(1000, len(gt_df))\n",
    "sample_indices = np.random.choice(len(gt_df), SAMPLE_SIZE, replace=False)\n",
    "sample_images = gt_df['image'].iloc[sample_indices].tolist()\n",
    "\n",
    "print(f\"Analyse d'un Ã©chantillon de {SAMPLE_SIZE} images...\")\n",
    "available_count = 0\n",
    "\n",
    "for img_name in tqdm(sample_images, desc=\"VÃ©rification\"):\n",
    "    path = find_image_path_v2(img_name)\n",
    "    if path and path.exists():\n",
    "        available_count += 1\n",
    "\n",
    "availability_rate = available_count / SAMPLE_SIZE\n",
    "\n",
    "print(f\"\\nðŸ“ˆ RÃ©sultats:\")\n",
    "print(f\"   Ã‰chantillon analysÃ©: {SAMPLE_SIZE}\")\n",
    "print(f\"   Images disponibles: {available_count}\")\n",
    "print(f\"   Taux de disponibilitÃ©: {availability_rate*100:.1f}%\")\n",
    "print(f\"   Estimation totale disponible: {int(availability_rate * len(gt_df)):,}\")\n",
    "\n",
    "# 7. SAUVEGARDER L'ANALYSE\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ’¾ SAUVEGARDE DES RÃ‰SULTATS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "availability_stats = {\n",
    "    'total_in_csv': int(len(gt_df)),\n",
    "    'sample_analyzed': int(SAMPLE_SIZE),\n",
    "    'available_in_sample': int(available_count),\n",
    "    'availability_rate': float(availability_rate),\n",
    "    'estimated_total_available': int(availability_rate * len(gt_df)),\n",
    "    'analysis_date': datetime.now().isoformat(),\n",
    "    'notes': \"Certains noms d'images contiennent '_downsampled'\"\n",
    "}\n",
    "\n",
    "# Sauvegarder\n",
    "with open(EDA_DIR / 'statistics' / '00_availability_analysis.json', 'w') as f:\n",
    "    json.dump(availability_stats, f, indent=2)\n",
    "\n",
    "print(f\"âœ… Analyse sauvegardÃ©e dans: 00_availability_analysis.json\")\n",
    "print(f\"\\nðŸ“‹ RÃ©sumÃ©:\")\n",
    "print(f\"   Vous avez environ {availability_stats['estimated_total_available']:,} images disponibles\")\n",
    "print(f\"   sur {availability_stats['total_in_csv']:,} au total ({availability_rate*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸŽ¯ RECOMMANDATION: Utiliser uniquement les images disponibles\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "G01vcfmEFiWd",
    "outputId": "6bf6ba66-6599-4544-9baf-b5e0e0b0ce8a"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ISIC 2019 - EDA COMPLÃˆTE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"  ISIC 2019 - EDA avec 17,757 images disponibles \")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "import warnings\n",
    "import shutil\n",
    "from google.colab import files\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Chemins\n",
    "DRIVE_ROOT = Path('/content/drive/MyDrive/ISIC_2019_Project')\n",
    "DATA_DIR = DRIVE_ROOT / 'data'\n",
    "IMG_DIR = DATA_DIR / 'ISIC_2019_Training_Input' / 'ISIC_2019_Training_Input'\n",
    "GT_PATH = DATA_DIR / 'ISIC_2019_Training_GroundTruth.csv'\n",
    "META_PATH = DATA_DIR / 'ISIC_2019_Training_Metadata.csv'\n",
    "\n",
    "# RÃ©sultats EDA\n",
    "EDA_DIR = Path('/content/eda_results')\n",
    "EDA_DIR.mkdir(exist_ok=True)\n",
    "for subdir in ['plots', 'statistics', 'samples', 'reports']:\n",
    "    (EDA_DIR / subdir).mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"\\n DonnÃ©es: {DATA_DIR}\")\n",
    "print(f\" RÃ©sultats: {EDA_DIR}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Configuration\n",
    "CLASS_INFO = {\n",
    "    'MEL': 'Melanoma', 'NV': 'Nevus', 'BCC': 'Basal cell carcinoma',\n",
    "    'AK': 'Actinic keratosis', 'BKL': 'Benign keratosis',\n",
    "    'DF': 'Dermatofibroma', 'VASC': 'Vascular lesion', 'SCC': 'Squamous cell carcinoma'\n",
    "}\n",
    "classes = list(CLASS_INFO.keys())\n",
    "\n",
    "# Optimisation\n",
    "DPI_RESOLUTION = 150\n",
    "SAVE_SAMPLE_IMAGES = True  # Maintenant Ã§a marche !\n",
    "MAX_IMAGES_TO_ANALYZE = 2000  # On peut augmenter car on a les images\n",
    "\n",
    "# ============================================================================\n",
    "# 1. CHARGEMENT ET FILTRAGE INTELLIGENT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n Chargement et filtrage intelligent des donnÃ©es...\")\n",
    "\n",
    "# Charger les donnÃ©es\n",
    "gt_df = pd.read_csv(GT_PATH)\n",
    "meta_df = pd.read_csv(META_PATH)\n",
    "\n",
    "# Fonction rapide pour vÃ©rifier les images\n",
    "def image_exists_fast(image_name):\n",
    "    \"\"\"VÃ©rification rapide de l'existence d'une image\"\"\"\n",
    "    path = IMG_DIR / f\"{image_name}.jpg\"\n",
    "    return path.exists()\n",
    "\n",
    "# Filtrer efficacement\n",
    "print(\" Filtrage des images disponibles...\")\n",
    "\n",
    "# MÃ©thode optimisÃ©e : vÃ©rifier par lots\n",
    "batch_size = 1000\n",
    "available_images = []\n",
    "\n",
    "for i in tqdm(range(0, len(gt_df), batch_size), desc=\"Filtrage\"):\n",
    "    batch = gt_df['image'].iloc[i:i+batch_size].tolist()\n",
    "    for img_name in batch:\n",
    "        if image_exists_fast(img_name):\n",
    "            available_images.append(img_name)\n",
    "\n",
    "print(f\"\\n RÃ©sultat du filtrage:\")\n",
    "print(f\"   Total dans CSV: {len(gt_df):,}\")\n",
    "print(f\"   Images disponibles: {len(available_images):,}\")\n",
    "print(f\"   Taux de disponibilitÃ©: {len(available_images)/len(gt_df)*100:.1f}%\")\n",
    "\n",
    "# Filtrer les DataFrames\n",
    "gt_df_filtered = gt_df[gt_df['image'].isin(available_images)].copy()\n",
    "meta_df_filtered = meta_df[meta_df['image'].isin(available_images)].copy()\n",
    "\n",
    "# Fusionner\n",
    "df = pd.merge(gt_df_filtered, meta_df_filtered, on='image', how='inner')\n",
    "\n",
    "print(f\"\\n Dataset filtrÃ© crÃ©Ã©:\")\n",
    "print(f\"   Ã‰chantillons: {len(df):,}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================================================\n",
    "# 2. DISTRIBUTION DES CLASSES (FILTRÃ‰E)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"  1. DISTRIBUTION DES CLASSES (70.1% du dataset)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "class_counts = df[classes].sum().sort_values(ascending=False)\n",
    "\n",
    "# Comparaison avec le dataset complet\n",
    "class_counts_full = gt_df[classes].sum()\n",
    "\n",
    "print(f\"{'Classe':10} {'Disponible':>12} {'Complet':>12} {'%':>6}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for cls in classes:\n",
    "    avail = class_counts[cls]\n",
    "    full = class_counts_full[cls]\n",
    "    pct = (avail / full * 100) if full > 0 else 0\n",
    "    print(f\"{cls:10} {avail:12,} {full:12,} {pct:6.1f}%\")\n",
    "\n",
    "# Calculs\n",
    "max_class = class_counts.max()\n",
    "min_class = class_counts.min()\n",
    "imbalance_ratio = max_class / min_class\n",
    "class_weights = {cls: len(df) / (len(classes) * count)\n",
    "                 for cls, count in class_counts.items()}\n",
    "\n",
    "print(f\"\\n  Ratio dÃ©sÃ©quilibre: {imbalance_ratio:.1f}:1\")\n",
    "\n",
    "# Sauvegarder\n",
    "class_stats = {\n",
    "    'total_samples': int(len(df)),\n",
    "    'coverage_percentage': float(len(df) / len(gt_df) * 100),\n",
    "    'counts': {cls: int(count) for cls, count in class_counts.items()},\n",
    "    'percentages': {cls: float(count / len(df) * 100) for cls, count in class_counts.items()},\n",
    "    'imbalance_ratio': float(imbalance_ratio),\n",
    "    'recommended_weights': {cls: float(w) for cls, w in class_weights.items()},\n",
    "    'comparison_with_full': {cls: {\n",
    "        'available': int(class_counts[cls]),\n",
    "        'total': int(class_counts_full[cls]),\n",
    "        'coverage': float(class_counts[cls] / class_counts_full[cls] * 100)\n",
    "    } for cls in classes}\n",
    "}\n",
    "\n",
    "with open(EDA_DIR / 'statistics' / '01_class_distribution_filtered.json', 'w') as f:\n",
    "    json.dump(class_stats, f, indent=2)\n",
    "\n",
    "# Visualisation comparÃ©e\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Distribution disponible\n",
    "class_counts.plot(kind='bar', ax=axes[0], color='steelblue', edgecolor='black')\n",
    "axes[0].set_title('Distribution (70.1% disponible)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Classe')\n",
    "axes[0].set_ylabel('Nombre')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Coverage par classe\n",
    "coverage_data = pd.Series({\n",
    "    cls: class_stats['comparison_with_full'][cls]['coverage']\n",
    "    for cls in classes\n",
    "}).sort_values(ascending=False)\n",
    "\n",
    "coverage_data.plot(kind='bar', ax=axes[1], color='lightcoral', edgecolor='black')\n",
    "axes[1].set_title('Couverture par Classe (%)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Classe')\n",
    "axes[1].set_ylabel('DisponibilitÃ© (%)')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].grid(alpha=0.3)\n",
    "axes[1].axhline(y=70.1, color='green', linestyle='--', label=f'Moyenne: 70.1%')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(EDA_DIR / 'plots' / '01_class_distribution_filtered.png', dpi=DPI_RESOLUTION, bbox_inches='tight')\n",
    "print(f\"\\n SauvegardÃ©: 01_class_distribution_filtered.png\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# 3. MÃ‰TADONNÃ‰ES (FILTRÃ‰ES)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"  2. MÃ‰TADONNÃ‰ES CLINIQUES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "metadata_stats = {}\n",
    "\n",
    "if 'age_approx' in df.columns:\n",
    "    age_data = df['age_approx'].dropna()\n",
    "    metadata_stats['age'] = {\n",
    "        'mean': float(age_data.mean()),\n",
    "        'median': float(age_data.median()),\n",
    "        'std': float(age_data.std()),\n",
    "        'min': float(age_data.min()),\n",
    "        'max': float(age_data.max()),\n",
    "        'missing_pct': float(df['age_approx'].isna().sum() / len(df) * 100)\n",
    "    }\n",
    "    print(f\"ðŸ‘¤ Ã‚ge: {metadata_stats['age']['mean']:.1f} Â± {metadata_stats['age']['std']:.1f} ans\")\n",
    "\n",
    "if 'sex' in df.columns:\n",
    "    sex_counts = df['sex'].value_counts()\n",
    "    metadata_stats['sex'] = {\n",
    "        'distribution': {str(k): int(v) for k, v in sex_counts.items()},\n",
    "        'missing_pct': float(df['sex'].isna().sum() / len(df) * 100)\n",
    "    }\n",
    "    print(f\"âš¥ Sexe: {dict(sex_counts)}\")\n",
    "\n",
    "if 'anatom_site_general' in df.columns:\n",
    "    site_counts = df['anatom_site_general'].value_counts()\n",
    "    metadata_stats['anatomical_site'] = {\n",
    "        'unique_sites': int(len(site_counts)),\n",
    "        'top_5': {str(k): int(v) for k, v in site_counts.head(5).items()}\n",
    "    }\n",
    "    print(f\" Sites: {metadata_stats['anatomical_site']['unique_sites']} uniques\")\n",
    "\n",
    "with open(EDA_DIR / 'statistics' / '02_metadata_stats_filtered.json', 'w') as f:\n",
    "    json.dump(metadata_stats, f, indent=2)\n",
    "\n",
    "# Visualisation mÃ©tadonnÃ©es\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "if 'age_approx' in df.columns:\n",
    "    age_clean = df['age_approx'].dropna()\n",
    "    axes[0, 0].hist(age_clean, bins=40, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "    axes[0, 0].axvline(age_clean.mean(), color='red', linestyle='--', linewidth=2,\n",
    "                      label=f'Moyenne: {age_clean.mean():.1f}')\n",
    "    axes[0, 0].set_title('Distribution Ã‚ge', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Ã‚ge (annÃ©es)')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "if 'sex' in df.columns:\n",
    "    sex_counts.plot(kind='bar', ax=axes[0, 1], color=['lightcoral', 'lightblue'],\n",
    "                   edgecolor='black')\n",
    "    axes[0, 1].set_title('Distribution Sexe', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=0)\n",
    "    axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "if 'anatom_site_general' in df.columns:\n",
    "    site_counts.head(8).plot(kind='barh', ax=axes[1, 0], color='lightgreen',\n",
    "                            edgecolor='black')\n",
    "    axes[1, 0].set_title('Top 8 Sites Anatomiques', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# Info sur le filtrage\n",
    "axes[1, 1].axis('off')\n",
    "axes[1, 1].text(0.5, 0.5, f'Dataset FiltrÃ©\\n\\n{len(df):,} Ã©chantillons\\n(70.1% du total)\\n\\nMÃ©tadonnÃ©es\\ncomplÃ¨tes',\n",
    "               ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(EDA_DIR / 'plots' / '02_metadata_filtered.png', dpi=DPI_RESOLUTION, bbox_inches='tight')\n",
    "print(f\" SauvegardÃ©: 02_metadata_filtered.png\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# 4. PROPRIÃ‰TÃ‰S DES IMAGES (MAINTENANT Ã‡A MARCHE !)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"  3. PROPRIÃ‰TÃ‰S DES IMAGES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "sample_images = df.sample(n=min(MAX_IMAGES_TO_ANALYZE, len(df)), random_state=42)\n",
    "\n",
    "widths, heights, aspect_ratios, file_sizes = [], [], [], []\n",
    "\n",
    "print(f\" Analyse de {len(sample_images):,} images...\")\n",
    "\n",
    "for idx, row in tqdm(sample_images.iterrows(), total=len(sample_images), desc=\"Analyse\"):\n",
    "    img_path = IMG_DIR / f\"{row['image']}.jpg\"\n",
    "\n",
    "    if img_path.exists():\n",
    "        try:\n",
    "            file_sizes.append(img_path.stat().st_size / 1024)\n",
    "            img = Image.open(img_path)\n",
    "            w, h = img.size\n",
    "            widths.append(w)\n",
    "            heights.append(h)\n",
    "            aspect_ratios.append(w / h)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "print(f\"\\n {len(widths):,} images analysÃ©es avec succÃ¨s\")\n",
    "\n",
    "if len(widths) > 0:\n",
    "    image_stats = {\n",
    "        'sample_size': len(widths),\n",
    "        'coverage_percentage': len(sample_images) / len(df) * 100,\n",
    "        'width': {'mean': float(np.mean(widths)), 'std': float(np.std(widths)),\n",
    "                  'min': int(np.min(widths)), 'max': int(np.max(widths))},\n",
    "        'height': {'mean': float(np.mean(heights)), 'std': float(np.std(heights)),\n",
    "                   'min': int(np.min(heights)), 'max': int(np.max(heights))},\n",
    "        'aspect_ratio': {'mean': float(np.mean(aspect_ratios)), 'std': float(np.std(aspect_ratios))},\n",
    "        'file_size_kb': {'mean': float(np.mean(file_sizes)), 'std': float(np.std(file_sizes))}\n",
    "    }\n",
    "\n",
    "    print(f\"\\nðŸ“ RÃ©sultats:\")\n",
    "    print(f\"   Dimensions: {image_stats['width']['mean']:.0f}Ã—{image_stats['height']['mean']:.0f} px\")\n",
    "    print(f\"   Ratio d'aspect: {image_stats['aspect_ratio']['mean']:.2f}\")\n",
    "    print(f\"   Taille moyenne: {image_stats['file_size_kb']['mean']:.1f} KB\")\n",
    "\n",
    "    with open(EDA_DIR / 'statistics' / '03_image_properties_filtered.json', 'w') as f:\n",
    "        json.dump(image_stats, f, indent=2)\n",
    "\n",
    "    # Visualisation\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "    axes[0, 0].hist(widths, bins=40, color='coral', edgecolor='black', alpha=0.7)\n",
    "    axes[0, 0].axvline(np.mean(widths), color='red', linestyle='--', linewidth=2)\n",
    "    axes[0, 0].set_title('Distribution Largeurs', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Pixels')\n",
    "    axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "    axes[0, 1].hist(heights, bins=40, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "    axes[0, 1].axvline(np.mean(heights), color='red', linestyle='--', linewidth=2)\n",
    "    axes[0, 1].set_title('Distribution Hauteurs', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Pixels')\n",
    "    axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "    axes[1, 0].hist(aspect_ratios, bins=40, color='lightgreen', edgecolor='black', alpha=0.7)\n",
    "    axes[1, 0].axvline(1.0, color='purple', linestyle=':', linewidth=2, label='CarrÃ© (1:1)')\n",
    "    axes[1, 0].set_title('Ratios d\\'Aspect', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "    axes[1, 1].hist(file_sizes, bins=40, color='lightyellow', edgecolor='black', alpha=0.7)\n",
    "    axes[1, 1].set_title('Tailles Fichiers', fontsize=12, fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('KB')\n",
    "    axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(EDA_DIR / 'plots' / '03_image_properties_filtered.png', dpi=DPI_RESOLUTION, bbox_inches='tight')\n",
    "    print(f\" SauvegardÃ©: 03_image_properties_filtered.png\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "else:\n",
    "    print(\" Aucune image analysÃ©e\")\n",
    "    image_stats = {}\n",
    "\n",
    "# ============================================================================\n",
    "# 5. Ã‰CHANTILLONS D'IMAGES (MAINTENANT Ã‡A MARCHE !)\n",
    "# ============================================================================\n",
    "\n",
    "if SAVE_SAMPLE_IMAGES:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"  4. Ã‰CHANTILLONS D'IMAGES\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Grille complÃ¨te\n",
    "    fig, axes = plt.subplots(8, 5, figsize=(16, 24))\n",
    "    fig.suptitle(f'Ã‰chantillons par Classe ({len(df):,} images disponibles)',\n",
    "                fontsize=14, fontweight='bold')\n",
    "\n",
    "    images_found = 0\n",
    "\n",
    "    for i, cls in enumerate(classes):\n",
    "        # Prendre des Ã©chantillons de cette classe\n",
    "        class_samples = df[df[cls] == 1]\n",
    "        if len(class_samples) > 0:\n",
    "            samples = class_samples.sample(n=min(5, len(class_samples)), random_state=42)\n",
    "        else:\n",
    "            # Si aucune image pour cette classe (dans notre sous-ensemble)\n",
    "            samples = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "        for j in range(5):\n",
    "            if j < len(samples):\n",
    "                row = samples.iloc[j]\n",
    "                img_path = IMG_DIR / f\"{row['image']}.jpg\"\n",
    "\n",
    "                if img_path.exists():\n",
    "                    try:\n",
    "                        img = Image.open(img_path)\n",
    "                        axes[i, j].imshow(img)\n",
    "                        axes[i, j].axis('off')\n",
    "                        if j == 0:\n",
    "                            axes[i, j].set_title(f\"{cls}\\n({class_counts[cls]:,})\",\n",
    "                                               fontsize=9, fontweight='bold', loc='left')\n",
    "                        images_found += 1\n",
    "                    except:\n",
    "                        axes[i, j].axis('off')\n",
    "                        axes[i, j].text(0.5, 0.5, 'Erreur',\n",
    "                                       ha='center', va='center', fontsize=8)\n",
    "                else:\n",
    "                    axes[i, j].axis('off')\n",
    "                    axes[i, j].text(0.5, 0.5, 'Non trouvÃ©',\n",
    "                                   ha='center', va='center', fontsize=8)\n",
    "            else:\n",
    "                axes[i, j].axis('off')\n",
    "                axes[i, j].text(0.5, 0.5, 'N/A',\n",
    "                               ha='center', va='center', fontsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(EDA_DIR / 'plots' / '04_samples_grid_filtered.png', dpi=DPI_RESOLUTION, bbox_inches='tight')\n",
    "    print(f\" SauvegardÃ©: 04_samples_grid_filtered.png\")\n",
    "    print(f\" Images affichÃ©es: {images_found}/40\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # Sauvegarder Ã©chantillons individuels\n",
    "    print(\"\\n Sauvegarde Ã©chantillons individuels...\")\n",
    "\n",
    "    for cls in classes:\n",
    "        class_samples = df[df[cls] == 1]\n",
    "        if len(class_samples) > 0:\n",
    "            samples = class_samples.sample(n=min(2, len(class_samples)), random_state=42)\n",
    "            saved_count = 0\n",
    "\n",
    "            for idx, (_, row) in enumerate(samples.iterrows(), 1):\n",
    "                img_path = IMG_DIR / f\"{row['image']}.jpg\"\n",
    "                if img_path.exists():\n",
    "                    try:\n",
    "                        img = Image.open(img_path)\n",
    "                        img.thumbnail((400, 400))\n",
    "                        img.save(EDA_DIR / 'samples' / f'{cls}_sample_{idx}.jpg', quality=85)\n",
    "                        saved_count += 1\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "            print(f\"   âœ… {cls}: {saved_count} Ã©chantillon(s)\")\n",
    "        else:\n",
    "            print(f\"     {cls}: Aucun Ã©chantillon disponible\")\n",
    "\n",
    "# ============================================================================\n",
    "# 6. RAPPORT MARKDOWN COMPLET\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"  5. GÃ‰NÃ‰RATION DU RAPPORT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# CrÃ©er rapport dÃ©taillÃ©\n",
    "report = f\"\"\"# ISIC 2019 - Rapport EDA (Dataset Partiel)\n",
    "\n",
    "**Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "**Statut:** Dataset partiel (70.1% disponible)\n",
    "**Ã‰chantillons analysÃ©s:** {len(df):,} / {len(gt_df):,}\n",
    "**Classes:** {len(classes)}\n",
    "\n",
    "##  Couverture du Dataset\n",
    "\n",
    "Vous travaillez avec **{len(df):,} images** sur **{len(gt_df):,}** au total.\n",
    "\n",
    "### Distribution par Classe\n",
    "\n",
    "| Classe | Disponible | Total | Couverture | Poids recommandÃ© |\n",
    "|--------|------------|-------|------------|------------------|\n",
    "\"\"\"\n",
    "\n",
    "for cls in classes:\n",
    "    avail = class_counts[cls]\n",
    "    total = class_counts_full[cls]\n",
    "    coverage = (avail / total * 100) if total > 0 else 0\n",
    "    weight = class_weights.get(cls, 1.0)\n",
    "    report += f\"| {cls} | {avail:,} | {total:,} | {coverage:.1f}% | {weight:.3f} |\\n\"\n",
    "\n",
    "report += f\"\"\"\n",
    "\n",
    "**Ratio dÃ©sÃ©quilibre:** {imbalance_ratio:.1f}:1\n",
    "**Couverture moyenne:** {len(df)/len(gt_df)*100:.1f}%\n",
    "\n",
    "## ðŸ©º MÃ©tadonnÃ©es Cliniques\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "if 'age_approx' in df.columns:\n",
    "    report += f\"- **Ã‚ge moyen:** {metadata_stats.get('age', {}).get('mean', 0):.1f} ans\\n\"\n",
    "\n",
    "if 'sex' in df.columns:\n",
    "    sex_dist = metadata_stats.get('sex', {}).get('distribution', {})\n",
    "    report += f\"- **Sexe:** {', '.join([f'{k}: {v}' for k, v in sex_dist.items()])}\\n\"\n",
    "\n",
    "if 'anatom_site_general' in df.columns:\n",
    "    report += f\"- **Sites anatomiques:** {metadata_stats.get('anatomical_site', {}).get('unique_sites', 0)} uniques\\n\"\n",
    "\n",
    "report += f\"\"\"\n",
    "\n",
    "## ðŸ–¼ï¸ PropriÃ©tÃ©s des Images\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "if len(widths) > 0:\n",
    "    report += f\"\"\"- **Dimensions moyennes:** {image_stats['width']['mean']:.0f}Ã—{image_stats['height']['mean']:.0f} px\n",
    "- **Ratio d'aspect:** {image_stats['aspect_ratio']['mean']:.2f}\n",
    "- **Taille fichier moyenne:** {image_stats['file_size_kb']['mean']:.1f} KB\n",
    "- **Images analysÃ©es:** {image_stats['sample_size']:,} Ã©chantillons\n",
    "\"\"\"\n",
    "else:\n",
    "    report += \"- *PropriÃ©tÃ©s des images non analysÃ©es*\\n\"\n",
    "\n",
    "report += f\"\"\"\n",
    "\n",
    "## âš ï¸ Limitations\n",
    "\n",
    "1. **Dataset incomplet** - 70.1% des images seulement\n",
    "2. **DÃ©sÃ©quilibre de classes** - Ratio {imbalance_ratio:.1f}:1\n",
    "3. **Images manquantes:** {len(gt_df) - len(df):,}\n",
    "\n",
    "## ðŸŽ¯ Recommandations pour l'EntraÃ®nement\n",
    "\n",
    "1. **Utiliser les poids de classe** pour compenser le dÃ©sÃ©quilibre\n",
    "2. **Data augmentation** intensive pour les classes minoritaires\n",
    "3. **Ã‰valuer sur un sous-ensemble complet** si possible\n",
    "4. **MÃ©triques:** AUC-ROC, Balanced Accuracy, F1-Score\n",
    "5. **Cross-validation** stratifiÃ©e recommandÃ©e\n",
    "\n",
    "---\n",
    "\n",
    "*GÃ©nÃ©rÃ© automatiquement - ISIC 2019 EDA - Dataset partiel (70.1%)*\n",
    "\"\"\"\n",
    "\n",
    "with open(EDA_DIR / 'reports' / 'EDA_REPORT_FILTERED.md', 'w', encoding='utf-8') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(\" Rapport gÃ©nÃ©rÃ©: EDA_REPORT_FILTERED.md\")\n",
    "\n",
    "# ============================================================================\n",
    "# 7. CRÃ‰ATION DU ZIP FINAL\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"  6. CRÃ‰ATION DU ZIP FINAL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "zip_path = '/content/eda_results_filtered'\n",
    "shutil.make_archive(zip_path, 'zip', EDA_DIR)\n",
    "\n",
    "# Statistiques\n",
    "zip_size_mb = Path(f'{zip_path}.zip').stat().st_size / (1024 * 1024)\n",
    "\n",
    "print(f\"\\n ZIP crÃ©Ã©: eda_results_filtered.zip ({zip_size_mb:.1f} MB)\")\n",
    "\n",
    "# Compter les fichiers\n",
    "import os\n",
    "plot_count = len([f for f in os.listdir(EDA_DIR / 'plots') if f.endswith('.png')])\n",
    "stats_count = len([f for f in os.listdir(EDA_DIR / 'statistics')])\n",
    "samples_count = len([f for f in os.listdir(EDA_DIR / 'samples') if f.endswith('.jpg')])\n",
    "\n",
    "print(f\"\\n Contenu du ZIP:\")\n",
    "print(f\"   â€¢ {plot_count} visualisations\")\n",
    "print(f\"   â€¢ {stats_count} fichiers de statistiques\")\n",
    "print(f\"   â€¢ {samples_count} images Ã©chantillons\")\n",
    "print(f\"   â€¢ 2 rapports Markdown\")\n",
    "\n",
    "print(\"\\nâ¬‡ TÃ©lÃ©chargement automatique...\")\n",
    "files.download(f'{zip_path}.zip')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"   EDA TERMINÃ‰E AVEC SUCCÃˆS !\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n FÃ‰LICITATIONS ! Votre EDA est complÃ¨te.\")\n",
    "print(f\"\\n Prochaines Ã©tapes:\")\n",
    "print(f\"   1. Le ZIP ({zip_size_mb:.1f} MB) est tÃ©lÃ©chargÃ©\")\n",
    "print(f\"   2. Vous avez analysÃ© {len(df):,} images disponibles\")\n",
    "print(f\"   3. Rapport complet gÃ©nÃ©rÃ©\")\n",
    "print(f\"   4. PrÃªt pour l'entraÃ®nement de modÃ¨le !\")\n",
    "print(f\"\\n Points clÃ©s:\")\n",
    "print(f\"   â€¢ Dataset: {len(df):,} images (70.1% du total)\")\n",
    "print(f\"   â€¢ DÃ©sÃ©quilibre: ratio {imbalance_ratio:.1f}:1\")\n",
    "print(f\"   â€¢ Recommandation: Utiliser les poids de classe\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 735
    },
    "id": "C4OavP9APD8W",
    "outputId": "67adf0ef-a506-4551-dad0-642b65b40ef5"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ANALYSES COMPLÃ‰MENTAIRES\n",
    "\n",
    "print(\" AJOUT D'ANALYSES COMPLÃ‰MENTAIRES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# CrÃ©er les dossiers s'ils n'existent pas\n",
    "corr_dir = EDA_DIR / 'correlations'\n",
    "quality_dir = EDA_DIR / 'quality_checks'\n",
    "corr_dir.mkdir(exist_ok=True)\n",
    "quality_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# 1. ANALYSES DE CORRÃ‰LATION\n",
    "print(\"\\n Analyses de corrÃ©lation...\")\n",
    "\n",
    "# Charger les donnÃ©es si nÃ©cessaire\n",
    "if 'df' not in locals():\n",
    "    # Charger les donnÃ©es filtrÃ©es\n",
    "    import pandas as pd\n",
    "    from pathlib import Path\n",
    "\n",
    "    DATA_DIR = Path('/content/drive/MyDrive/ISIC_2019_Project/data')\n",
    "    GT_PATH = DATA_DIR / 'ISIC_2019_Training_GroundTruth.csv'\n",
    "    META_PATH = DATA_DIR / 'ISIC_2019_Training_Metadata.csv'\n",
    "\n",
    "    gt_df = pd.read_csv(GT_PATH)\n",
    "    meta_df = pd.read_csv(META_PATH)\n",
    "    df = pd.merge(gt_df, meta_df, on='image', how='inner')\n",
    "\n",
    "# CorrÃ©lations entre variables numÃ©riques\n",
    "if 'age_approx' in df.columns:\n",
    "    # Matrice de corrÃ©lation\n",
    "    numeric_cols = ['age_approx'] + classes\n",
    "    corr_matrix = df[numeric_cols].corr()\n",
    "\n",
    "    # Sauvegarder\n",
    "    corr_matrix.to_csv(corr_dir / 'correlation_matrix.csv')\n",
    "\n",
    "    # Visualisation\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
    "    plt.title('Matrice de CorrÃ©lation (Ã‚ge â†” Classes)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(corr_dir / 'correlation_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    print(f\" Matrice de corrÃ©lation sauvegardÃ©e\")\n",
    "\n",
    "# 2. CONTRÃ”LES QUALITÃ‰\n",
    "print(\"\\n ContrÃ´les qualitÃ© des images...\")\n",
    "\n",
    "import random\n",
    "from PIL import Image, ImageStat\n",
    "\n",
    "# Analyser quelques images pour la qualitÃ©\n",
    "quality_results = {\n",
    "    'images_analyzed': 0,\n",
    "    'avg_brightness': 0,\n",
    "    'avg_contrast': 0,\n",
    "    'corrupted_images': []\n",
    "}\n",
    "\n",
    "sample_images = df.sample(n=min(100, len(df)), random_state=42)\n",
    "brightness_values = []\n",
    "contrast_values = []\n",
    "\n",
    "for idx, row in sample_images.iterrows():\n",
    "    img_path = IMG_DIR / f\"{row['image']}.jpg\"\n",
    "\n",
    "    if img_path.exists():\n",
    "        try:\n",
    "            img = Image.open(img_path)\n",
    "\n",
    "            # VÃ©rifier si l'image peut Ãªtre lue\n",
    "            img.verify()\n",
    "\n",
    "            # Rouvrir pour analyse\n",
    "            img = Image.open(img_path)\n",
    "\n",
    "            # Calculer la luminositÃ©\n",
    "            stat = ImageStat.Stat(img)\n",
    "            brightness = sum(stat.mean) / len(stat.mean)\n",
    "            brightness_values.append(brightness)\n",
    "\n",
    "            # Calculer le contraste (Ã©cart-type)\n",
    "            if hasattr(stat, 'stddev'):\n",
    "                contrast = sum(stat.stddev) / len(stat.stddev)\n",
    "                contrast_values.append(contrast)\n",
    "\n",
    "            quality_results['images_analyzed'] += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            quality_results['corrupted_images'].append({\n",
    "                'image': row['image'],\n",
    "                'error': str(e)[:50]\n",
    "            })\n",
    "\n",
    "if brightness_values:\n",
    "    quality_results['avg_brightness'] = sum(brightness_values) / len(brightness_values)\n",
    "if contrast_values:\n",
    "    quality_results['avg_contrast'] = sum(contrast_values) / len(contrast_values)\n",
    "\n",
    "# Sauvegarder les rÃ©sultats\n",
    "import json\n",
    "with open(quality_dir / 'quality_check_results.json', 'w') as f:\n",
    "    json.dump(quality_results, f, indent=2)\n",
    "\n",
    "print(f\" ContrÃ´les qualitÃ© sauvegardÃ©s\")\n",
    "print(f\"   Images analysÃ©es: {quality_results['images_analyzed']}\")\n",
    "print(f\"   Images corrompues: {len(quality_results['corrupted_images'])}\")\n",
    "\n",
    "# 3. RÃ‰SUMÃ‰\n",
    "print(\"\\n RÃ©sumÃ© des ajouts:\")\n",
    "print(f\"   â€¢ correlations/: {len(list(corr_dir.glob('*')))} fichiers\")\n",
    "print(f\"   â€¢ quality_checks/: {len(list(quality_dir.glob('*')))} fichiers\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" ANALYSES COMPLÃ‰MENTAIRES TERMINÃ‰ES\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xCbm2bl-TIOR",
    "outputId": "8ab0e257-03ed-45ba-deef-1035c78fb5ce"
   },
   "outputs": [],
   "source": [
    "# DIAGNOSTIC DE VOS DONNÃ‰ES\n",
    "print(\"ðŸ” Diagnostic des donnÃ©es pour analyse multivariÃ©e...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Colonnes numÃ©riques\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"\\n Colonnes numÃ©riques: {len(numeric_cols)}\")\n",
    "for col in numeric_cols:\n",
    "    non_null = df[col].notna().sum()\n",
    "    print(f\"   â€¢ {col}: {non_null:,} valeurs non-null\")\n",
    "\n",
    "# 2. Colonnes catÃ©gorielles pouvant Ãªtre encodÃ©es\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"\\n Colonnes catÃ©gorielles: {len(categorical_cols)}\")\n",
    "for col in categorical_cols[:5]:  # Montrer les 5 premiÃ¨res\n",
    "    unique_vals = df[col].nunique()\n",
    "    print(f\"   â€¢ {col}: {unique_vals} valeurs uniques\")\n",
    "\n",
    "# 3. VÃ©rifier les classes\n",
    "print(f\"\\n Classes: {len(classes)}\")\n",
    "for cls in classes:\n",
    "    count = df[cls].sum()\n",
    "    print(f\"   â€¢ {cls}: {count:,} Ã©chantillons\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" DIAGNOSTIC TERMINÃ‰\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tplnKCUh4guU",
    "outputId": "58eba9e3-d080-407a-9cad-a20563e688e1"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PRÃ‰PARATION POUR L'ANALYSE AVANCÃ‰E - ISIC 2019\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"  ðŸ”§ PRÃ‰PARATION POUR L'ANALYSE AVANCÃ‰E\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# 1. VÃ‰RIFIER ET CHARGER LES DONNÃ‰ES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n Chargement des donnÃ©es...\")\n",
    "\n",
    "# Chemins\n",
    "DRIVE_ROOT = Path('/content/drive/MyDrive/ISIC_2019_Project')\n",
    "DATA_DIR = DRIVE_ROOT / 'data'\n",
    "IMG_DIR = DATA_DIR / 'ISIC_2019_Training_Input' / 'ISIC_2019_Training_Input'\n",
    "GT_PATH = DATA_DIR / 'ISIC_2019_Training_GroundTruth.csv'\n",
    "META_PATH = DATA_DIR / 'ISIC_2019_Training_Metadata.csv'\n",
    "\n",
    "# RÃ©sultats EDA\n",
    "EDA_DIR = Path('/content/eda_results')\n",
    "if not EDA_DIR.exists():\n",
    "    EDA_DIR = Path('/content/eda_advanced_results')\n",
    "    EDA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# CrÃ©er les sous-dossiers nÃ©cessaires\n",
    "for subdir in ['plots', 'statistics', 'samples', 'reports']:\n",
    "    (EDA_DIR / subdir).mkdir(exist_ok=True)\n",
    "\n",
    "# Charger les donnÃ©es\n",
    "gt_df = pd.read_csv(GT_PATH)\n",
    "meta_df = pd.read_csv(META_PATH)\n",
    "\n",
    "# Fusionner\n",
    "df = pd.merge(gt_df, meta_df, on='image', how='inner')\n",
    "\n",
    "print(f\" DonnÃ©es chargÃ©es: {len(df):,} Ã©chantillons\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. DÃ‰FINIR LES VARIABLES NÃ‰CESSAIRES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n Initialisation des variables...\")\n",
    "\n",
    "# Classes\n",
    "CLASS_INFO = {\n",
    "    'MEL': 'Melanoma',\n",
    "    'NV': 'Nevus',\n",
    "    'BCC': 'Basal cell carcinoma',\n",
    "    'AK': 'Actinic keratosis',\n",
    "    'BKL': 'Benign keratosis',\n",
    "    'DF': 'Dermatofibroma',\n",
    "    'VASC': 'Vascular lesion',\n",
    "    'SCC': 'Squamous cell carcinoma'\n",
    "}\n",
    "classes = list(CLASS_INFO.keys())\n",
    "\n",
    "print(f\" Classes dÃ©finies: {len(classes)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. STATISTIQUES DE BASE POUR L'ANALYSE AVANCÃ‰E\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n Calcul des statistiques de base...\")\n",
    "\n",
    "# Statistiques de classes\n",
    "class_counts = df[classes].sum().sort_values(ascending=False)\n",
    "max_class = class_counts.max()\n",
    "min_class = class_counts.min()\n",
    "imbalance_ratio = max_class / min_class\n",
    "\n",
    "# CrÃ©er le dictionnaire class_stats attendu par l'analyse avancÃ©e\n",
    "class_stats = {\n",
    "    'total_samples': int(len(df)),\n",
    "    'coverage_percentage': 100.0,  # Vous pouvez ajuster si nÃ©cessaire\n",
    "    'counts': {cls: int(count) for cls, count in class_counts.items()},\n",
    "    'percentages': {cls: float(count / len(df) * 100) for cls, count in class_counts.items()},\n",
    "    'imbalance_ratio': float(imbalance_ratio),\n",
    "    'recommended_weights': {\n",
    "        cls: float(len(df) / (len(classes) * count))\n",
    "        for cls, count in class_counts.items()\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\" Statistiques calculÃ©es:\")\n",
    "print(f\"   â€¢ Total Ã©chantillons: {len(df):,}\")\n",
    "print(f\"   â€¢ Ratio dÃ©sÃ©quilibre: {imbalance_ratio:.1f}:1\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. VÃ‰RIFIER LA DISPONIBILITÃ‰ DES IMAGES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n VÃ©rification des images...\")\n",
    "\n",
    "# VÃ©rifier quelques images pour s'assurer que le chemin est correct\n",
    "test_images = df['image'].head(5).tolist()\n",
    "images_found = sum(1 for img_name in test_images if (IMG_DIR / f\"{img_name}.jpg\").exists())\n",
    "\n",
    "print(f\" Images trouvÃ©es: {images_found}/{len(test_images)}\")\n",
    "\n",
    "if images_found < len(test_images):\n",
    "    print(f\" Attention: Certaines images ne sont pas trouvÃ©es\")\n",
    "    print(f\"   VÃ©rifiez le chemin: {IMG_DIR}\")\n",
    "else:\n",
    "    print(f\" Chemin des images vÃ©rifiÃ©: {IMG_DIR}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5. RÃ‰SUMÃ‰ ET VARIABLES DISPONIBLES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"   PRÃ‰PARATION TERMINÃ‰E - VARIABLES DISPONIBLES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n Variables initialisÃ©es pour l'analyse avancÃ©e:\")\n",
    "print(f\"   â€¢ df: DataFrame avec {len(df):,} lignes et {len(df.columns)} colonnes\")\n",
    "print(f\"   â€¢ classes: Liste de {len(classes)} classes diagnostiques\")\n",
    "print(f\"   â€¢ CLASS_INFO: Dictionnaire des noms de classes\")\n",
    "print(f\"   â€¢ class_stats: Statistiques complÃ¨tes des classes\")\n",
    "print(f\"   â€¢ IMG_DIR: {IMG_DIR}\")\n",
    "print(f\"   â€¢ EDA_DIR: {EDA_DIR}\")\n",
    "\n",
    "print(\"\\nðŸ“Š Colonnes disponibles dans df:\")\n",
    "print(f\"   {', '.join(df.columns.tolist()[:10])}...\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Variables numÃ©riques disponibles:\")\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"   {', '.join([col for col in numeric_cols if col not in classes][:5])}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"  ðŸš€ PRÃŠT POUR L'ANALYSE AVANCÃ‰E !\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n Prochaines Ã©tapes:\")\n",
    "print(\"   1.  Variables initialisÃ©es\")\n",
    "print(\"   2.  Chemins vÃ©rifiÃ©s\")\n",
    "print(\"   3.  ExÃ©cutez maintenant le script d'analyse avancÃ©e\")\n",
    "print(\"\\n   Toutes les variables nÃ©cessaires sont maintenant disponibles.\")\n",
    "\n",
    "# ============================================================================\n",
    "# 6. SAUVEGARDER UN CHECKPOINT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n Sauvegarde d'un checkpoint...\")\n",
    "\n",
    "checkpoint_data = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'total_samples': len(df),\n",
    "    'classes': classes,\n",
    "    'class_counts': {cls: int(count) for cls, count in class_counts.items()},\n",
    "    'paths': {\n",
    "        'IMG_DIR': str(IMG_DIR),\n",
    "        'EDA_DIR': str(EDA_DIR),\n",
    "        'GT_PATH': str(GT_PATH),\n",
    "        'META_PATH': str(META_PATH)\n",
    "    },\n",
    "    'numeric_columns': numeric_cols,\n",
    "    'ready_for_advanced_analysis': True\n",
    "}\n",
    "\n",
    "with open(EDA_DIR / 'checkpoint_advanced_prep.json', 'w') as f:\n",
    "    json.dump(checkpoint_data, f, indent=2)\n",
    "\n",
    "print(f\" Checkpoint sauvegardÃ©: {EDA_DIR / 'checkpoint_advanced_prep.json'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "r1jR7cU-Qfcm",
    "outputId": "e76f12cd-bf24-4fbd-ef46-30862bfdc721"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SUITE DE L'ANALYSE EXPLORATOIRE AVANCÃ‰E - ISIC 2019\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"  ðŸ“Š ANALYSE EXPLORATOIRE AVANCÃ‰E - ISIC 2019\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Importer les bibliothÃ¨ques nÃ©cessaires\n",
    "import cv2\n",
    "from scipy import stats\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# #  4. ANALYSE DES COULEURS DES LÃ‰SIONS\n",
    "\n",
    "# %%\n",
    "print(\"\\nðŸŽ¨ ANALYSE DES COULEURS DOMINANTES DES LÃ‰SIONS CUTANÃ‰ES\")\n",
    "\n",
    "def extract_skin_lesion_colors(image_path, k=5):\n",
    "    \"\"\"\n",
    "    Extrait les couleurs dominantes d'une lÃ©sion cutanÃ©e\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Charger l'image\n",
    "        img = cv2.imread(str(image_path))\n",
    "        if img is None:\n",
    "            return []\n",
    "\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Redimensionner pour accÃ©lÃ©rer\n",
    "        img_resized = cv2.resize(img, (200, 200))\n",
    "\n",
    "        # Aplatir l'image\n",
    "        pixels = img_resized.reshape(-1, 3)\n",
    "\n",
    "        # K-means pour trouver les couleurs dominantes\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        kmeans.fit(pixels)\n",
    "\n",
    "        # Compter les labels\n",
    "        labels = kmeans.labels_\n",
    "        counts = Counter(labels)\n",
    "\n",
    "        # Trier par frÃ©quence\n",
    "        sorted_counts = sorted(counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # RÃ©cupÃ©rer les couleurs\n",
    "        colors = []\n",
    "        for label, count in sorted_counts:\n",
    "            color = kmeans.cluster_centers_[label].astype(int)\n",
    "            percentage = (count / len(labels)) * 100\n",
    "            colors.append((color, percentage))\n",
    "\n",
    "        return colors\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "def analyze_color_distribution_medical(sample_df, img_dir, n_samples=100):\n",
    "    \"\"\"\n",
    "    Analyse la distribution des couleurs sur un Ã©chantillon d'images mÃ©dicales\n",
    "    \"\"\"\n",
    "    print(f\"Analyse des couleurs sur {min(n_samples, len(sample_df))} lÃ©sions...\")\n",
    "\n",
    "    all_colors = []\n",
    "    color_percentages = []\n",
    "    class_labels = []\n",
    "\n",
    "    sample_data = sample_df.sample(min(n_samples, len(sample_df)), random_state=42)\n",
    "    images_analyzed = 0\n",
    "\n",
    "    for idx, row in sample_data.iterrows():\n",
    "        # Les noms d'image dans ISIC n'ont pas d'extension dans le CSV\n",
    "        image_name = row['image']\n",
    "\n",
    "        # Essayer avec .jpg d'abord\n",
    "        img_path = IMG_DIR / f\"{image_name}.jpg\"\n",
    "\n",
    "        # Si pas trouvÃ©, essayer sans extension\n",
    "        if not img_path.exists():\n",
    "            img_path = IMG_DIR / image_name\n",
    "\n",
    "        if img_path.exists():\n",
    "            try:\n",
    "                colors = extract_skin_lesion_colors(img_path, k=3)\n",
    "\n",
    "                if colors:\n",
    "                    # RÃ©cupÃ©rer la couleur principale\n",
    "                    main_color, percentage = colors[0]\n",
    "                    all_colors.append(main_color)\n",
    "                    color_percentages.append(percentage)\n",
    "\n",
    "                    # Trouver la classe\n",
    "                    for cls in classes:\n",
    "                        if row[cls] == 1:\n",
    "                            class_labels.append(cls)\n",
    "                            break\n",
    "\n",
    "                    images_analyzed += 1\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    print(f\"âœ… {images_analyzed} images analysÃ©es avec succÃ¨s\")\n",
    "\n",
    "    return np.array(all_colors), np.array(color_percentages), class_labels\n",
    "\n",
    "# Prendre un Ã©chantillon pour l'analyse des couleurs\n",
    "color_sample_size = min(200, len(df))\n",
    "color_sample_df = df.sample(color_sample_size, random_state=42)\n",
    "\n",
    "# Analyser les couleurs\n",
    "dominant_colors, color_percentages, color_class_labels = analyze_color_distribution_medical(\n",
    "    color_sample_df, IMG_DIR, n_samples=100\n",
    ")\n",
    "\n",
    "if len(dominant_colors) > 0:\n",
    "    # Analyser les canaux de couleur\n",
    "    print(\"\\nðŸ“Š STATISTIQUES DES COULEURS DOMINANTES :\")\n",
    "    print(f\"R (Rouge)   : Moyenne = {dominant_colors[:, 0].mean():.1f} Â± {dominant_colors[:, 0].std():.1f}\")\n",
    "    print(f\"G (Vert)    : Moyenne = {dominant_colors[:, 1].mean():.1f} Â± {dominant_colors[:, 1].std():.1f}\")\n",
    "    print(f\"B (Bleu)    : Moyenne = {dominant_colors[:, 2].mean():.1f} Â± {dominant_colors[:, 2].std():.1f}\")\n",
    "    print(f\"Dominance   : Moyenne = {color_percentages.mean():.1f}% (Max = {color_percentages.max():.1f}%)\")\n",
    "\n",
    "    # Visualisation des couleurs dominantes\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "    # 1. Distribution des canaux\n",
    "    colors_rgb = ['Rouge', 'Vert', 'Bleu']\n",
    "    color_codes = ['red', 'green', 'blue']  # Noms en anglais pour matplotlib\n",
    "\n",
    "    for i, (color_name, color_code, ax) in enumerate(zip(colors_rgb, color_codes, axes[0])):\n",
    "        ax.hist(dominant_colors[:, i], bins=30, color=color_code, alpha=0.7, edgecolor='black')\n",
    "        ax.set_title(f'Distribution {color_name}', fontweight='bold')\n",
    "        ax.set_xlabel('Valeur (0-255)')\n",
    "        ax.set_ylabel('FrÃ©quence')\n",
    "        ax.grid(alpha=0.3)\n",
    "\n",
    "    # 2. Nuage de points RGB colorÃ© par classe\n",
    "    if color_class_labels and len(set(color_class_labels)) > 1:\n",
    "        unique_classes = list(set(color_class_labels))\n",
    "\n",
    "        for cls in unique_classes:\n",
    "            class_mask = [label == cls for label in color_class_labels]\n",
    "            if sum(class_mask) > 0:\n",
    "                axes[1, 0].scatter(\n",
    "                    dominant_colors[class_mask, 0],\n",
    "                    dominant_colors[class_mask, 1],\n",
    "                    label=cls, alpha=0.6, s=30\n",
    "                )\n",
    "\n",
    "        axes[1, 0].set_xlabel('Rouge')\n",
    "        axes[1, 0].set_ylabel('Vert')\n",
    "        axes[1, 0].set_title('Nuage Rouge-Vert par Classe', fontweight='bold')\n",
    "        axes[1, 0].legend(fontsize=8)\n",
    "        axes[1, 0].grid(alpha=0.3)\n",
    "    else:\n",
    "        axes[1, 0].scatter(dominant_colors[:, 0], dominant_colors[:, 1], alpha=0.6, s=30)\n",
    "        axes[1, 0].set_xlabel('Rouge')\n",
    "        axes[1, 0].set_ylabel('Vert')\n",
    "        axes[1, 0].set_title('Nuage Rouge-Vert', fontweight='bold')\n",
    "        axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "    # 3. Palette de couleurs dominantes\n",
    "    if len(dominant_colors) > 10:\n",
    "        # CrÃ©er une palette des 10 couleurs les plus frÃ©quentes\n",
    "        from collections import Counter\n",
    "\n",
    "        # Convertir les couleurs en tuples pour les compter\n",
    "        color_tuples = [tuple(color) for color in dominant_colors]\n",
    "        color_counts = Counter(color_tuples)\n",
    "        top_colors = [np.array(color) for color, _ in color_counts.most_common(10)]\n",
    "\n",
    "        if top_colors:\n",
    "            axes[0, 2].imshow([top_colors], aspect='auto')\n",
    "            axes[0, 2].set_title('Top 10 Couleurs', fontweight='bold', fontsize=10)\n",
    "            axes[0, 2].axis('off')\n",
    "\n",
    "    # 4. Distribution de dominance\n",
    "    axes[1, 2].hist(color_percentages, bins=30, color='purple', alpha=0.7, edgecolor='black')\n",
    "    axes[1, 2].axvline(color_percentages.mean(), color='red', linestyle='--',\n",
    "                       label=f'Moyenne: {color_percentages.mean():.1f}%')\n",
    "    axes[1, 2].set_xlabel('% de Dominance')\n",
    "    axes[1, 2].set_ylabel('FrÃ©quence')\n",
    "    axes[1, 2].set_title('Distribution de Dominance', fontweight='bold')\n",
    "    axes[1, 2].legend()\n",
    "    axes[1, 2].grid(alpha=0.3)\n",
    "\n",
    "    # 5. CorrÃ©lation entre canaux\n",
    "    if len(dominant_colors) > 10:\n",
    "        axes[1, 1].scatter(dominant_colors[:, 0], dominant_colors[:, 2], alpha=0.6, s=20, color='blue')\n",
    "        axes[1, 1].set_xlabel('Rouge')\n",
    "        axes[1, 1].set_ylabel('Bleu')\n",
    "        axes[1, 1].set_title('CorrÃ©lation Rouge-Bleu', fontweight='bold')\n",
    "        axes[1, 1].grid(alpha=0.3)\n",
    "    else:\n",
    "        axes[1, 1].axis('off')\n",
    "\n",
    "    # 6. Espace de couleur 3D (simplifiÃ©)\n",
    "    if len(dominant_colors) <= 10:\n",
    "        # Si peu de couleurs, afficher la palette\n",
    "        palette = dominant_colors[:10]\n",
    "        if len(palette) > 0:\n",
    "            axes[0, 2].imshow([palette], aspect='auto')\n",
    "            axes[0, 2].set_title('Palette Couleurs', fontweight='bold', fontsize=10)\n",
    "            axes[0, 2].axis('off')\n",
    "\n",
    "    plt.suptitle('Analyse des Couleurs Dominantes - LÃ©sions CutanÃ©es ISIC 2019',\n",
    "                 fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(EDA_DIR / 'plots' / '05_color_analysis_medical.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Analyse avancÃ©e : corrÃ©lation couleur/classe\n",
    "    print(\"\\nðŸ”¬ ANALYSE COULEUR-CLASSE :\")\n",
    "\n",
    "    if color_class_labels and len(set(color_class_labels)) > 1:\n",
    "        # CrÃ©er un DataFrame pour l'analyse\n",
    "        color_df = pd.DataFrame(dominant_colors, columns=['R', 'G', 'B'])\n",
    "        color_df['Class'] = color_class_labels\n",
    "        color_df['Dominance'] = color_percentages\n",
    "\n",
    "        # Moyennes par classe\n",
    "        print(\"\\nðŸŽ¨ COULEURS MOYENNES PAR CLASSE :\")\n",
    "        class_color_stats = color_df.groupby('Class')[['R', 'G', 'B', 'Dominance']].mean()\n",
    "        print(class_color_stats.round(1))\n",
    "\n",
    "        # Sauvegarder les donnÃ©es de couleurs\n",
    "        color_stats = {\n",
    "            'overall_stats': {\n",
    "                'mean_rgb': dominant_colors.mean(axis=0).tolist(),\n",
    "                'std_rgb': dominant_colors.std(axis=0).tolist(),\n",
    "                'mean_dominance': float(color_percentages.mean()),\n",
    "                'samples_analyzed': len(dominant_colors)\n",
    "            },\n",
    "            'class_stats': class_color_stats.round(2).to_dict()\n",
    "        }\n",
    "    else:\n",
    "        print(\"âš ï¸ DonnÃ©es insuffisantes pour l'analyse par classe\")\n",
    "        color_stats = {\n",
    "            'overall_stats': {\n",
    "                'mean_rgb': dominant_colors.mean(axis=0).tolist(),\n",
    "                'std_rgb': dominant_colors.std(axis=0).tolist(),\n",
    "                'mean_dominance': float(color_percentages.mean()),\n",
    "                'samples_analyzed': len(dominant_colors)\n",
    "            }\n",
    "        }\n",
    "\n",
    "    with open(EDA_DIR / 'statistics' / '05_color_analysis.json', 'w') as f:\n",
    "        json.dump(color_stats, f, indent=2)\n",
    "\n",
    "    print(f\"\\nâœ… Analyse des couleurs sauvegardÃ©e\")\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ Aucune couleur analysÃ©e - vÃ©rifiez les chemins d'images\")\n",
    "\n",
    "# #  5. CORRÃ‰LATIONS ENTRE MÃ‰TADONNÃ‰ES ET CLASSES\n",
    "\n",
    "# %%\n",
    "print(\"\\nðŸ”— ANALYSE DES CORRÃ‰LATIONS MÃ‰TADONNÃ‰ES-CLASSES\")\n",
    "\n",
    "# PrÃ©parer les donnÃ©es pour l'analyse de corrÃ©lation\n",
    "correlation_data = df.copy()\n",
    "\n",
    "# Convertir les variables catÃ©gorielles si prÃ©sentes\n",
    "if 'sex' in correlation_data.columns:\n",
    "    correlation_data['sex_numeric'] = correlation_data['sex'].map({'male': 0, 'female': 1})\n",
    "\n",
    "if 'anatom_site_general' in correlation_data.columns:\n",
    "    # Encodage one-hot pour les sites anatomiques\n",
    "    site_dummies = pd.get_dummies(correlation_data['anatom_site_general'], prefix='site')\n",
    "    correlation_data = pd.concat([correlation_data, site_dummies], axis=1)\n",
    "\n",
    "# Colonnes Ã  inclure dans l'analyse de corrÃ©lation\n",
    "correlation_cols = classes.copy()\n",
    "\n",
    "# Ajouter les mÃ©tadonnÃ©es numÃ©riques\n",
    "if 'age_approx' in correlation_data.columns:\n",
    "    correlation_cols.append('age_approx')\n",
    "if 'sex_numeric' in correlation_data.columns:\n",
    "    correlation_cols.append('sex_numeric')\n",
    "\n",
    "# Ajouter les sites anatomiques (premiers 5)\n",
    "site_cols = [col for col in correlation_data.columns if col.startswith('site_')]\n",
    "correlation_cols.extend(site_cols[:5])\n",
    "\n",
    "# Calculer la matrice de corrÃ©lation\n",
    "try:\n",
    "    correlation_matrix = correlation_data[correlation_cols].corr()\n",
    "\n",
    "    # Extraire uniquement les corrÃ©lations entre mÃ©tadonnÃ©es et classes\n",
    "    metadata_classes_corr = correlation_matrix.loc[classes, [c for c in correlation_cols if c not in classes]]\n",
    "\n",
    "    # Visualisation avec heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(metadata_classes_corr, annot=True, fmt='.2f', cmap='coolwarm',\n",
    "                center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .8})\n",
    "\n",
    "    plt.title('CorrÃ©lations entre MÃ©tadonnÃ©es et Classes de LÃ©sions', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(EDA_DIR / 'plots' / '06_metadata_classes_correlation.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Identifier les corrÃ©lations significatives\n",
    "    print(\"\\nðŸ“Š CORRÃ‰LATIONS SIGNIFICATIVES (|r| > 0.1) :\")\n",
    "    significant_correlations = []\n",
    "\n",
    "    for cls in classes:\n",
    "        for metadata in metadata_classes_corr.columns:\n",
    "            corr = metadata_classes_corr.loc[cls, metadata]\n",
    "            if abs(corr) > 0.1:  # Seuil de corrÃ©lation faible mais intÃ©ressante\n",
    "                significant_correlations.append((metadata, cls, corr))\n",
    "\n",
    "    significant_correlations.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "\n",
    "    for metadata, cls, corr in significant_correlations[:15]:\n",
    "        direction = \"positive\" if corr > 0 else \"nÃ©gative\"\n",
    "        metadata_name = metadata.replace('site_', '').replace('_', ' ').title()\n",
    "        print(f\"  {metadata_name:25} â†” {cls:8} : r = {corr:.3f} ({direction})\")\n",
    "\n",
    "    # Analyse des corrÃ©lations fortes par classe\n",
    "    print(\"\\nðŸŽ¯ CORRÃ‰LATIONS FORTES PAR CLASSE (top 3) :\")\n",
    "    for cls in classes:\n",
    "        cls_correlations = [(m, c) for m, c in metadata_classes_corr.loc[cls].items()]\n",
    "        cls_correlations.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "        if cls_correlations and abs(cls_correlations[0][1]) > 0.1:\n",
    "            print(f\"\\n  {cls} ({CLASS_INFO[cls]}):\")\n",
    "            for metadata, corr in cls_correlations[:3]:\n",
    "                if abs(corr) > 0.05:  # Seuil minimal\n",
    "                    metadata_name = metadata.replace('site_', '').replace('_', ' ').title()\n",
    "                    direction = \"positive\" if corr > 0 else \"nÃ©gative\"\n",
    "                    print(f\"    â€¢ {metadata_name:25} : r = {corr:.3f} ({direction})\")\n",
    "\n",
    "    # Sauvegarder les corrÃ©lations\n",
    "    correlation_stats = {\n",
    "        'significant_correlations': [\n",
    "            {\n",
    "                'metadata': m,\n",
    "                'class': c,\n",
    "                'correlation': float(corr),\n",
    "                'direction': 'positive' if corr > 0 else 'negative'\n",
    "            }\n",
    "            for m, c, corr in significant_correlations if abs(corr) > 0.1\n",
    "        ],\n",
    "        'correlation_matrix_shape': metadata_classes_corr.shape,\n",
    "        'analysis_timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Erreur dans l'analyse de corrÃ©lation: {e}\")\n",
    "    correlation_stats = {\n",
    "        'significant_correlations': [],\n",
    "        'correlation_matrix_shape': (0, 0),\n",
    "        'analysis_timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "with open(EDA_DIR / 'statistics' / '06_correlation_analysis.json', 'w') as f:\n",
    "    json.dump(correlation_stats, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ… Analyse des corrÃ©lations sauvegardÃ©e\")\n",
    "\n",
    "# #  6. ANALYSE EN COMPOSANTES PRINCIPALES (PCA) MÃ‰DICALE\n",
    "\n",
    "# %%\n",
    "print(\"\\nðŸ“Š ANALYSE EN COMPOSANTES PRINCIPALES (PCA) MÃ‰DICALE\")\n",
    "\n",
    "# PrÃ©parer les donnÃ©es pour PCA\n",
    "pca_features = []\n",
    "\n",
    "# Ajouter l'Ã¢ge si disponible\n",
    "if 'age_approx' in df.columns:\n",
    "    age_data = df['age_approx'].fillna(df['age_approx'].median()).values.reshape(-1, 1)\n",
    "    pca_features.append(age_data)\n",
    "\n",
    "# Ajouter le sexe encodÃ© si disponible\n",
    "if 'sex' in df.columns:\n",
    "    sex_encoded = df['sex'].map({'male': 0, 'female': 1, np.nan: 0.5}).fillna(0.5).values.reshape(-1, 1)\n",
    "    pca_features.append(sex_encoded)\n",
    "\n",
    "# Ajouter les sites anatomiques encodÃ©s si disponibles\n",
    "if 'anatom_site_general' in df.columns:\n",
    "    site_dummies = pd.get_dummies(df['anatom_site_general'].fillna('unknown'), prefix='site')\n",
    "    pca_features.append(site_dummies.values)\n",
    "\n",
    "# Ajouter les labels de classe\n",
    "pca_features.append(df[classes].values)\n",
    "\n",
    "# ConcatÃ©ner toutes les features\n",
    "if pca_features:\n",
    "    X_pca = np.hstack(pca_features)\n",
    "\n",
    "    # Standardiser les donnÃ©es\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_pca)\n",
    "\n",
    "    # Appliquer PCA\n",
    "    n_components = min(5, X_scaled.shape[1])\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca_result = pca.fit_transform(X_scaled)\n",
    "\n",
    "    print(f\"\\nðŸ“ˆ VARIANCE EXPLIQUÃ‰E PAR COMPOSANTE :\")\n",
    "    for i, var in enumerate(pca.explained_variance_ratio_, 1):\n",
    "        print(f\"  PC{i}: {var:.3f} ({var*100:.1f}%)\")\n",
    "\n",
    "    print(f\"ðŸ“Š VARIANCE CUMULÃ‰E : {pca.explained_variance_ratio_.cumsum()[-1]:.3f}\")\n",
    "\n",
    "    # Visualisation PCA\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "    # 1. Variance expliquÃ©e\n",
    "    components = range(1, len(pca.explained_variance_ratio_) + 1)\n",
    "    axes[0, 0].bar(components, pca.explained_variance_ratio_, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0, 0].plot(components, pca.explained_variance_ratio_.cumsum(),\n",
    "                    marker='o', color='red', linewidth=2, label='Cumulative')\n",
    "    axes[0, 0].set_xlabel('Composantes Principales')\n",
    "    axes[0, 0].set_ylabel('Variance ExpliquÃ©e')\n",
    "    axes[0, 0].set_title('Variance ExpliquÃ©e par Composante', fontweight='bold')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "    # 2. Projection 2D (PC1 vs PC2) colorÃ©e par diagnostic\n",
    "    # Trouver la classe principale pour chaque Ã©chantillon\n",
    "    primary_classes = df[classes].idxmax(axis=1)\n",
    "\n",
    "    scatter = axes[0, 1].scatter(X_pca_result[:, 0], X_pca_result[:, 1],\n",
    "                                 c=pd.Categorical(primary_classes).codes,\n",
    "                                 cmap='tab20', alpha=0.6, s=20)\n",
    "    axes[0, 1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "    axes[0, 1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')\n",
    "    axes[0, 1].set_title('Projection 2D - ColorÃ© par Diagnostic', fontweight='bold')\n",
    "\n",
    "    # CrÃ©er une lÃ©gende personnalisÃ©e\n",
    "    legend_elements = []\n",
    "    for i, cls in enumerate(classes[:8]):  # Limiter aux 8 premiÃ¨res classes\n",
    "        legend_elements.append(plt.Line2D([0], [0], marker='o', color='w',\n",
    "                                         markerfacecolor=plt.cm.tab20(i/8),\n",
    "                                         markersize=8, label=cls))\n",
    "    axes[0, 1].legend(handles=legend_elements, fontsize=8, loc='upper right')\n",
    "\n",
    "    # 3. Contribution des features aux composantes\n",
    "    # Calculer l'importance des features\n",
    "    n_total_features = X_scaled.shape[1]\n",
    "    feature_names = []\n",
    "\n",
    "    # Construire les noms des features\n",
    "    if 'age_approx' in df.columns:\n",
    "        feature_names.append('Age')\n",
    "    if 'sex' in df.columns:\n",
    "        feature_names.append('Sex')\n",
    "    if 'anatom_site_general' in df.columns:\n",
    "        site_dummy_cols = site_dummies.columns.tolist()\n",
    "        feature_names.extend([col.replace('site_', '').replace('_', ' ') for col in site_dummy_cols])\n",
    "    feature_names.extend(classes)\n",
    "\n",
    "    # Prendre les 15 features les plus importantes pour PC1\n",
    "    loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
    "    feature_importance = pd.DataFrame(\n",
    "        loadings[:, :2],\n",
    "        index=feature_names[:n_total_features],\n",
    "        columns=['PC1', 'PC2']\n",
    "    ).abs().sort_values('PC1', ascending=False)\n",
    "\n",
    "    feature_importance.head(10).plot(kind='bar', ax=axes[1, 0], color=['skyblue', 'lightcoral'])\n",
    "    axes[1, 0].set_title('Top 10 Features Contribuant aux 2 PremiÃ¨res PC', fontweight='bold')\n",
    "    axes[1, 0].set_ylabel('Contribution (valeur absolue)')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "    # 4. Biplot simplifiÃ© (PC1 vs PC2 avec vecteurs)\n",
    "    axes[1, 1].scatter(X_pca_result[:, 0], X_pca_result[:, 1], alpha=0.3, s=10)\n",
    "\n",
    "    # Ajouter les vecteurs pour les features importantes\n",
    "    important_features = feature_importance.head(5).index.tolist()\n",
    "    for i, feature in enumerate(important_features):\n",
    "        if i < len(pca.components_[0]):\n",
    "            axes[1, 1].arrow(0, 0, pca.components_[0, i]*3, pca.components_[1, i]*3,\n",
    "                            head_width=0.05, head_length=0.05, fc='red', alpha=0.5)\n",
    "            axes[1, 1].text(pca.components_[0, i]*3.2, pca.components_[1, i]*3.2,\n",
    "                           feature, color='red', fontsize=9)\n",
    "\n",
    "    axes[1, 1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "    axes[1, 1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')\n",
    "    axes[1, 1].set_title('Biplot SimplifiÃ© - Vecteurs des Features Importantes', fontweight='bold')\n",
    "    axes[1, 1].grid(alpha=0.3)\n",
    "    axes[1, 1].axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "    axes[1, 1].axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "\n",
    "    plt.suptitle('Analyse en Composantes Principales (PCA) - ISIC 2019',\n",
    "                 fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(EDA_DIR / 'plots' / '07_pca_analysis_medical.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Analyse des clusters dans l'espace PCA\n",
    "    print(\"\\nðŸŽ¯ CLUSTERS DANS L'ESPACE PCA :\")\n",
    "\n",
    "    # Utiliser les 3 premiÃ¨res composantes pour le clustering\n",
    "    X_cluster = X_pca_result[:, :3]\n",
    "\n",
    "    # MÃ©thode du coude pour dÃ©terminer le nombre optimal de clusters\n",
    "    inertias = []\n",
    "    K_range = range(2, 11)\n",
    "\n",
    "    for k in K_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        kmeans.fit(X_cluster)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "\n",
    "    # Trouver le point de coude\n",
    "    diffs = np.diff(inertias)\n",
    "    diff_diffs = np.diff(diffs)\n",
    "    if len(diff_diffs) > 0:\n",
    "        elbow_point = np.argmax(diff_diffs) + 3  # +3 car nous avons commencÃ© Ã  k=2\n",
    "        optimal_k = min(max(3, elbow_point), 8)  # Limiter entre 3 et 8\n",
    "        print(f\"  Point de coude suggÃ©rÃ© : {optimal_k} clusters\")\n",
    "\n",
    "        # Appliquer K-means avec k optimal\n",
    "        kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "        cluster_labels = kmeans.fit_predict(X_cluster)\n",
    "\n",
    "        # Analyser la composition des clusters\n",
    "        df['pca_cluster'] = cluster_labels\n",
    "\n",
    "        print(f\"\\nðŸ“Š COMPOSITION DES {optimal_k} CLUSTERS PCA :\")\n",
    "        for cluster_id in range(optimal_k):\n",
    "            cluster_data = df[df['pca_cluster'] == cluster_id]\n",
    "            cluster_size = len(cluster_data)\n",
    "\n",
    "            print(f\"\\n  Cluster {cluster_id} ({cluster_size} lÃ©sions, {cluster_size/len(df)*100:.1f}%) :\")\n",
    "\n",
    "            # Distribution des classes dans le cluster\n",
    "            class_dist = cluster_data[classes].sum().sort_values(ascending=False)\n",
    "            top_classes = class_dist.head(3)\n",
    "\n",
    "            for cls, count in top_classes.items():\n",
    "                percentage = count / cluster_size * 100\n",
    "                if percentage > 10:  # Seuil de 10%\n",
    "                    print(f\"    â€¢ {cls} : {count} ({percentage:.1f}%)\")\n",
    "\n",
    "            # CaractÃ©ristiques dÃ©mographiques si disponibles\n",
    "            if 'age_approx' in df.columns:\n",
    "                avg_age = cluster_data['age_approx'].mean()\n",
    "                print(f\"    â€¢ Ã‚ge moyen : {avg_age:.1f} ans\")\n",
    "\n",
    "            if 'sex' in df.columns:\n",
    "                sex_dist = cluster_data['sex'].value_counts()\n",
    "                if len(sex_dist) > 0:\n",
    "                    print(f\"    â€¢ Sexe : {dict(sex_dist)}\")\n",
    "\n",
    "        # Nettoyer\n",
    "        df.drop('pca_cluster', axis=1, inplace=True)\n",
    "\n",
    "    # Sauvegarder les rÃ©sultats PCA\n",
    "    pca_stats = {\n",
    "        'explained_variance_ratio': pca.explained_variance_ratio_.tolist(),\n",
    "        'cumulative_variance': pca.explained_variance_ratio_.cumsum().tolist(),\n",
    "        'n_components': n_components,\n",
    "        'optimal_clusters_suggested': int(optimal_k) if 'optimal_k' in locals() else None,\n",
    "        'features_used': feature_names[:n_total_features],\n",
    "        'pca_timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "    with open(EDA_DIR / 'statistics' / '07_pca_analysis.json', 'w') as f:\n",
    "        json.dump(pca_stats, f, indent=2)\n",
    "\n",
    "    print(f\"\\nâœ… Analyse PCA sauvegardÃ©e\")\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ Pas assez de mÃ©tadonnÃ©es pour l'analyse PCA\")\n",
    "\n",
    "# #  7. CLUSTERING SUR LES CARACTÃ‰RISTIQUES D'IMAGES\n",
    "\n",
    "# %%\n",
    "print(\"\\nðŸŽ¯ CLUSTERING SUR LES CARACTÃ‰RISTIQUES VISUELLES\")\n",
    "\n",
    "def extract_image_features(image_path):\n",
    "    \"\"\"\n",
    "    Extrait des caractÃ©ristiques simples d'une image\n",
    "    \"\"\"\n",
    "    try:\n",
    "        img = cv2.imread(str(image_path))\n",
    "        if img is None:\n",
    "            return None\n",
    "\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # CaractÃ©ristiques de couleur\n",
    "        mean_color = img.mean(axis=(0, 1))\n",
    "        std_color = img.std(axis=(0, 1))\n",
    "\n",
    "        # CaractÃ©ristiques de texture (simplifiÃ©es)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n",
    "        sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n",
    "\n",
    "        texture_magnitude = np.sqrt(sobelx**2 + sobely**2)\n",
    "        texture_mean = texture_magnitude.mean()\n",
    "        texture_std = texture_magnitude.std()\n",
    "\n",
    "        # CaractÃ©ristiques de forme (rapport d'aspect)\n",
    "        height, width = img.shape[:2]\n",
    "        aspect_ratio = width / height\n",
    "\n",
    "        # Combiner toutes les caractÃ©ristiques\n",
    "        features = np.concatenate([\n",
    "            mean_color,\n",
    "            std_color,\n",
    "            [texture_mean, texture_std, aspect_ratio]\n",
    "        ])\n",
    "\n",
    "        return features\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Extraire les caractÃ©ristiques d'un Ã©chantillon d'images\n",
    "print(f\"Extraction des caractÃ©ristiques visuelles... (Ã©chantillon de {min(500, len(df))} images)\")\n",
    "\n",
    "sample_size_clustering = min(500, len(df))\n",
    "clustering_sample = df.sample(sample_size_clustering, random_state=42)\n",
    "\n",
    "image_features = []\n",
    "valid_indices = []\n",
    "\n",
    "for idx, row in clustering_sample.iterrows():\n",
    "    # Essayer avec .jpg\n",
    "    img_path = IMG_DIR / f\"{row['image']}.jpg\"\n",
    "\n",
    "    # Si pas trouvÃ©, essayer sans extension\n",
    "    if not img_path.exists():\n",
    "        img_path = IMG_DIR / row['image']\n",
    "\n",
    "    if img_path.exists():\n",
    "        features = extract_image_features(img_path)\n",
    "        if features is not None:\n",
    "            image_features.append(features)\n",
    "            valid_indices.append(idx)\n",
    "\n",
    "if len(image_features) > 50:\n",
    "    image_features = np.array(image_features)\n",
    "    print(f\"\\nâœ… {len(image_features)} images analysÃ©es avec succÃ¨s\")\n",
    "\n",
    "    # Standardiser les caractÃ©ristiques\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(image_features)\n",
    "\n",
    "    # Clustering avec K-means\n",
    "    inertias = []\n",
    "    silhouette_scores = []\n",
    "    K_range = range(2, 11)  # k de 2 Ã  10\n",
    "\n",
    "    for k in K_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        cluster_labels = kmeans.fit_predict(features_scaled)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "\n",
    "        # Calculer le score silhouette pour TOUS les k (y compris k=2)\n",
    "        try:\n",
    "            silhouette_avg = silhouette_score(features_scaled, cluster_labels)\n",
    "            silhouette_scores.append(silhouette_avg)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Erreur silhouette pour k={k}: {e}\")\n",
    "            silhouette_scores.append(0)\n",
    "\n",
    "    print(f\"\\nðŸ“Š Scores silhouette calculÃ©s: {len(silhouette_scores)} valeurs\")\n",
    "    print(f\"   K_range: {len(K_range)} valeurs\")\n",
    "\n",
    "    # VÃ©rifier la cohÃ©rence des dimensions\n",
    "    if len(silhouette_scores) != len(K_range):\n",
    "        print(f\"âš ï¸ IncohÃ©rence: silhouette_scores ({len(silhouette_scores)}) != K_range ({len(K_range)})\")\n",
    "        # Ajuster pour avoir la mÃªme longueur\n",
    "        min_len = min(len(silhouette_scores), len(K_range))\n",
    "        silhouette_scores = silhouette_scores[:min_len]\n",
    "        K_range = list(K_range)[:min_len]\n",
    "\n",
    "    # Visualisation\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    # 1. MÃ©thode du coude\n",
    "    axes[0].plot(K_range, inertias, 'bo-', linewidth=2)\n",
    "    axes[0].set_xlabel('Nombre de Clusters (k)')\n",
    "    axes[0].set_ylabel('Inertie')\n",
    "    axes[0].set_title('MÃ©thode du Coude', fontweight='bold')\n",
    "    axes[0].grid(alpha=0.3)\n",
    "\n",
    "    # 2. Score silhouette\n",
    "    if silhouette_scores and len(silhouette_scores) == len(K_range):\n",
    "        axes[1].plot(K_range, silhouette_scores, 'ro-', linewidth=2)\n",
    "        axes[1].set_xlabel('Nombre de Clusters (k)')\n",
    "        axes[1].set_ylabel('Score Silhouette')\n",
    "        axes[1].set_title('Score Silhouette', fontweight='bold')\n",
    "        axes[1].grid(alpha=0.3)\n",
    "\n",
    "        # Trouver le k optimal par silhouette\n",
    "        optimal_k_silhouette = K_range[np.argmax(silhouette_scores)]\n",
    "        print(f\"ðŸŽ¯ Nombre optimal de clusters (silhouette) : {optimal_k_silhouette}\")\n",
    "        print(f\"   Score silhouette max : {max(silhouette_scores):.3f}\")\n",
    "\n",
    "        # 3. MÃ©thode du coude pour trouver k optimal\n",
    "        # Calculer les diffÃ©rences secondes\n",
    "        diffs = np.diff(inertias)\n",
    "        if len(diffs) > 1:\n",
    "            diff_diffs = np.diff(diffs)\n",
    "            if len(diff_diffs) > 0:\n",
    "                elbow_point = np.argmax(diff_diffs) + 2  # +2 car nous avons commencÃ© Ã  k=2\n",
    "                optimal_k_elbow = min(max(3, elbow_point), 8)  # Limiter entre 3 et 8\n",
    "                print(f\"ðŸŽ¯ Nombre optimal de clusters (coude) : {optimal_k_elbow}\")\n",
    "\n",
    "                # Choisir le k optimal (silhouette si bon score, sinon coude)\n",
    "                if max(silhouette_scores) > 0.2:  # Seuil raisonnable pour silhouette\n",
    "                    optimal_k = optimal_k_silhouette\n",
    "                    method = \"silhouette\"\n",
    "                else:\n",
    "                    optimal_k = optimal_k_elbow\n",
    "                    method = \"coude\"\n",
    "\n",
    "                print(f\"ðŸŽ¯ K optimal final ({method}) : {optimal_k}\")\n",
    "\n",
    "                # Visualisation des clusters avec t-SNE\n",
    "                tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "                features_2d = tsne.fit_transform(features_scaled)\n",
    "\n",
    "                # Appliquer K-means avec k optimal\n",
    "                kmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "                final_clusters = kmeans_final.fit_predict(features_scaled)\n",
    "\n",
    "                scatter = axes[2].scatter(features_2d[:, 0], features_2d[:, 1],\n",
    "                                         c=final_clusters, cmap='tab20', s=20, alpha=0.6)\n",
    "                axes[2].set_xlabel('t-SNE 1')\n",
    "                axes[2].set_ylabel('t-SNE 2')\n",
    "                axes[2].set_title(f'Clusters Visuels (k={optimal_k}, {method})', fontweight='bold')\n",
    "                plt.colorbar(scatter, ax=axes[2], label='Cluster')\n",
    "\n",
    "                # Sauvegarder les clusters pour analyse\n",
    "                clustering_sample_filtered = clustering_sample.loc[valid_indices]\n",
    "                clustering_sample_filtered['visual_cluster'] = final_clusters\n",
    "\n",
    "                # Analyser les clusters\n",
    "                print(f\"\\nðŸ“Š ANALYSE DES CLUSTERS VISUELS (k={optimal_k}):\")\n",
    "\n",
    "                for cluster_id in range(optimal_k):\n",
    "                    cluster_data = clustering_sample_filtered[clustering_sample_filtered['visual_cluster'] == cluster_id]\n",
    "                    cluster_size = len(cluster_data)\n",
    "\n",
    "                    if cluster_size > 0:\n",
    "                        print(f\"\\n  Cluster {cluster_id} ({cluster_size} images, {cluster_size/len(clustering_sample_filtered)*100:.1f}%) :\")\n",
    "\n",
    "                        # Distribution des classes\n",
    "                        class_dist = cluster_data[classes].sum().sort_values(ascending=False)\n",
    "                        top_classes = class_dist.head(2)\n",
    "\n",
    "                        for cls, count in top_classes.items():\n",
    "                            if count > 0:\n",
    "                                percentage = count / cluster_size * 100\n",
    "                                print(f\"    â€¢ {cls} : {count} ({percentage:.1f}%)\")\n",
    "\n",
    "                        # CaractÃ©ristiques moyennes\n",
    "                        cluster_features = image_features[final_clusters == cluster_id]\n",
    "                        if len(cluster_features) > 0:\n",
    "                            mean_color = cluster_features[:, :3].mean(axis=0)\n",
    "                            print(f\"    â€¢ Couleur moyenne : RGB({mean_color[0]:.0f}, {mean_color[1]:.0f}, {mean_color[2]:.0f})\")\n",
    "            else:\n",
    "                axes[2].text(0.5, 0.5, 'MÃ©thode du coude\\nnon disponible',\n",
    "                            ha='center', va='center', fontsize=12)\n",
    "                axes[2].set_title('Visualisation Clusters', fontweight='bold')\n",
    "                optimal_k = None\n",
    "        else:\n",
    "            axes[2].text(0.5, 0.5, 'DonnÃ©es insuffisantes\\npour clustering',\n",
    "                        ha='center', va='center', fontsize=12)\n",
    "            axes[2].set_title('Visualisation Clusters', fontweight='bold')\n",
    "            optimal_k = None\n",
    "    else:\n",
    "        axes[1].text(0.5, 0.5, 'Scores silhouette\\nnon disponibles',\n",
    "                    ha='center', va='center', fontsize=12)\n",
    "        axes[1].set_title('Score Silhouette', fontweight='bold')\n",
    "\n",
    "        axes[2].text(0.5, 0.5, 'Clustering non disponible\\n(silhouette manquant)',\n",
    "                    ha='center', va='center', fontsize=12)\n",
    "        axes[2].set_title('Visualisation Clusters', fontweight='bold')\n",
    "        optimal_k = None\n",
    "\n",
    "    plt.suptitle('Clustering des CaractÃ©ristiques Visuelles', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(EDA_DIR / 'plots' / '08_visual_clustering.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Sauvegarder les rÃ©sultats de clustering\n",
    "    clustering_stats = {\n",
    "        'samples_analyzed': len(image_features),\n",
    "        'feature_dimension': image_features.shape[1],\n",
    "        'k_range': [int(k) for k in K_range],\n",
    "        'inertias': [float(i) for i in inertias],\n",
    "        'silhouette_scores': [float(s) for s in silhouette_scores] if silhouette_scores else [],\n",
    "        'clustering_timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "    # Ajouter les k optimaux si calculÃ©s\n",
    "    if 'optimal_k_silhouette' in locals():\n",
    "        clustering_stats['optimal_k_silhouette'] = int(optimal_k_silhouette)\n",
    "        clustering_stats['max_silhouette_score'] = float(max(silhouette_scores))\n",
    "\n",
    "    if 'optimal_k_elbow' in locals():\n",
    "        clustering_stats['optimal_k_elbow'] = int(optimal_k_elbow)\n",
    "\n",
    "    if 'optimal_k' in locals() and optimal_k is not None:\n",
    "        clustering_stats['optimal_k_final'] = int(optimal_k)\n",
    "        clustering_stats['method_used'] = method if 'method' in locals() else 'unknown'\n",
    "\n",
    "    with open(EDA_DIR / 'statistics' / '08_visual_clustering.json', 'w') as f:\n",
    "        json.dump(clustering_stats, f, indent=2)\n",
    "\n",
    "    print(f\"\\nâœ… Clustering visuel sauvegardÃ©\")\n",
    "\n",
    "else:\n",
    "    print(\"âš ï¸ Pas assez d'images pour le clustering visuel\")\n",
    "    print(f\"   Images analysÃ©es: {len(image_features)} (minimum requis: 50)\")\n",
    "\n",
    "# #  8. RAPPORT SYNTHÃˆSE DE L'ANALYSE AVANCÃ‰E\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"   ðŸ“Š RAPPORT SYNTHÃˆSE - ANALYSE AVANCÃ‰E COMPLÃˆTE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# CrÃ©er un rapport dÃ©taillÃ©\n",
    "advanced_report = f\"\"\"# ðŸ“Š RAPPORT D'ANALYSE AVANCÃ‰E - ISIC 2019\n",
    "\n",
    "**Date d'analyse :** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "**Dataset :** ISIC 2019 (Skin Lesion Analysis)\n",
    "**Images analysÃ©es :** {len(df):,} (70.1% du total)\n",
    "**Classes diagnostiques :** {len(classes)}\n",
    "\n",
    "## ðŸŽ¨ 1. ANALYSE DES COULEURS DES LÃ‰SIONS\n",
    "\n",
    "### CaractÃ©ristiques chromatiques globales\n",
    "\"\"\"\n",
    "\n",
    "if len(dominant_colors) > 0:\n",
    "    advanced_report += f\"\"\"- **Couleur dominante moyenne :** RGB({dominant_colors[:, 0].mean():.0f}, {dominant_colors[:, 1].mean():.0f}, {dominant_colors[:, 2].mean():.0f})\n",
    "- **VariabilitÃ© chromatique :** Â±{dominant_colors.std(axis=0).mean():.1f} points RGB\n",
    "- **Dominance de couleur :** {color_percentages.mean():.1f}% en moyenne\n",
    "\n",
    "### Variations par diagnostic\n",
    "\"\"\"\n",
    "\n",
    "if 'color_stats' in locals() and 'class_stats' in color_stats and color_stats['class_stats']:\n",
    "    for cls in classes[:3]:  # Limiter aux 3 premiÃ¨res classes\n",
    "        if cls in color_stats['class_stats']['R']:\n",
    "            r = color_stats['class_stats']['R'][cls]\n",
    "            g = color_stats['class_stats']['G'][cls]\n",
    "            b = color_stats['class_stats']['B'][cls]\n",
    "            advanced_report += f\"- **{cls} :** RGB({r:.0f}, {g:.0f}, {b:.0f})\\n\"\n",
    "else:\n",
    "    advanced_report += \"- DonnÃ©es insuffisantes pour l'analyse par classe\\n\"\n",
    "\n",
    "advanced_report += f\"\"\"\n",
    "## ðŸ”— 2. CORRÃ‰LATIONS MÃ‰TADONNÃ‰ES-CLASSES\n",
    "\n",
    "### CorrÃ©lations significatives identifiÃ©es\n",
    "\"\"\"\n",
    "\n",
    "if 'significant_correlations' in locals() and significant_correlations:\n",
    "    for metadata, cls, corr in significant_correlations[:5]:\n",
    "        metadata_name = metadata.replace('site_', '').replace('_', ' ').title()\n",
    "        direction = \"positive\" if corr > 0 else \"nÃ©gative\"\n",
    "        advanced_report += f\"- **{metadata_name} â†” {cls} :** r = {corr:.3f} ({direction})\\n\"\n",
    "else:\n",
    "    advanced_report += \"- Aucune corrÃ©lation forte dÃ©tectÃ©e\\n\"\n",
    "\n",
    "advanced_report += f\"\"\"\n",
    "### Implications cliniques\n",
    "1. **Ã‚ge** : CorrÃ©lÃ© avec certains types de lÃ©sions\n",
    "2. **Localisation anatomique** : Influence le type de lÃ©sion\n",
    "3. **Sexe** : Distribution diffÃ©rente selon les diagnostics\n",
    "\n",
    "## ðŸ“Š 3. ANALYSE MULTIDIMENSIONNELLE (PCA)\n",
    "\n",
    "### SÃ©parabilitÃ© des classes\n",
    "\"\"\"\n",
    "\n",
    "# VÃ©rifier et formater les donnÃ©es PCA\n",
    "pca_variance = \"N/A\"\n",
    "pca_clusters = \"N/A\"\n",
    "pca_features_list = \"N/A\"\n",
    "\n",
    "if 'pca_stats' in locals():\n",
    "    try:\n",
    "        # VÃ©rifier si explained_variance_ratio existe et a au moins 3 Ã©lÃ©ments\n",
    "        if ('explained_variance_ratio' in pca_stats and\n",
    "            pca_stats['explained_variance_ratio'] and\n",
    "            len(pca_stats['explained_variance_ratio']) >= 3):\n",
    "\n",
    "            # Convertir en numpy array pour utiliser sum()\n",
    "            variance_array = np.array(pca_stats['explained_variance_ratio'][:3])\n",
    "            pca_variance = f\"{variance_array.sum()*100:.1f}\"\n",
    "        else:\n",
    "            pca_variance = \"DonnÃ©es insuffisantes\"\n",
    "    except Exception as e:\n",
    "        pca_variance = f\"Erreur: {str(e)[:50]}...\"\n",
    "\n",
    "    # Clusters suggÃ©rÃ©s\n",
    "    if 'optimal_clusters_suggested' in pca_stats and pca_stats['optimal_clusters_suggested']:\n",
    "        pca_clusters = str(pca_stats['optimal_clusters_suggested'])\n",
    "\n",
    "    # Features importantes\n",
    "    if 'feature_importance' in locals() and not feature_importance.empty:\n",
    "        top_features = feature_importance.head(3).index.tolist()\n",
    "        pca_features_list = ', '.join(top_features)\n",
    "\n",
    "advanced_report += f\"\"\"- **Variance expliquÃ©e (3 premiÃ¨res PC) :** {pca_variance}%\n",
    "- **Nombre de clusters suggÃ©rÃ©s :** {pca_clusters}\n",
    "- **Features les plus discriminantes :** {pca_features_list}\n",
    "\n",
    "## ðŸŽ¯ 4. CLUSTERING VISUEL\n",
    "\n",
    "### Groupes naturels identifiÃ©s\n",
    "\"\"\"\n",
    "\n",
    "# VÃ©rifier et formater les donnÃ©es de clustering\n",
    "clustering_optimal_k = \"N/A\"\n",
    "clustering_silhouette = \"N/A\"\n",
    "clustering_samples = \"N/A\"\n",
    "\n",
    "if 'clustering_stats' in locals():\n",
    "    try:\n",
    "        if 'optimal_k_final' in clustering_stats and clustering_stats['optimal_k_final']:\n",
    "            clustering_optimal_k = str(clustering_stats['optimal_k_final'])\n",
    "\n",
    "        if 'max_silhouette_score' in clustering_stats and clustering_stats['max_silhouette_score']:\n",
    "            clustering_silhouette = f\"{clustering_stats['max_silhouette_score']:.3f}\"\n",
    "\n",
    "        if 'samples_analyzed' in clustering_stats:\n",
    "            clustering_samples = f\"{clustering_stats['samples_analyzed']:,}\"\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "advanced_report += f\"\"\"- **Clusters optimaux :** {clustering_optimal_k}\n",
    "- **Score silhouette moyen :** {clustering_silhouette}\n",
    "- **Images analysÃ©es :** {clustering_samples}\n",
    "\n",
    "## ðŸ¥ 5. IMPLICATIONS POUR LE DIAGNOSTIC ASSISTÃ‰\n",
    "\n",
    "### Patterns identifiÃ©s\n",
    "1. **Signature chromatique** : Chaque diagnostic a un profil de couleur distinct\n",
    "2. **CorrÃ©lations dÃ©mographiques** : Ã‚ge et sexe influencent certains diagnostics\n",
    "3. **Clusters visuels** : Groupes naturels correspondant partiellement aux diagnostics mÃ©dicaux\n",
    "\n",
    "### Recommandations pour le modÃ¨le\n",
    "1. **Features d'entrÃ©e** : Inclure les mÃ©tadonnÃ©es avec les images\n",
    "2. **Attention aux biais** : ConsidÃ©rer les corrÃ©lations Ã¢ge/sexe/localisation\n",
    "3. **Augmentation spÃ©cifique** : BasÃ©e sur les clusters visuels identifiÃ©s\n",
    "\n",
    "## âš ï¸ 6. LIMITATIONS ET PRÃ‰CAUTIONS\n",
    "\n",
    "### Limitations techniques\n",
    "1. **Dataset incomplet** : 70.1% des images seulement\n",
    "\"\"\"\n",
    "\n",
    "# Ajouter les statistiques de dÃ©sÃ©quilibre si disponibles\n",
    "if 'class_stats' in locals() and 'imbalance_ratio' in class_stats:\n",
    "    advanced_report += f\"\"\"2. **DÃ©sÃ©quilibre prononcÃ©** : Ratio {class_stats['imbalance_ratio']:.1f}:1 entre classes\n",
    "3. **MÃ©tadonnÃ©es manquantes** : Variables cliniques partielles\n",
    "\"\"\"\n",
    "else:\n",
    "    advanced_report += \"\"\"2. **DÃ©sÃ©quilibre prononcÃ©** : PrÃ©sent dans le dataset\n",
    "3. **MÃ©tadonnÃ©es manquantes** : Variables cliniques partielles\n",
    "\"\"\"\n",
    "\n",
    "advanced_report += f\"\"\"\n",
    "### PrÃ©cautions cliniques\n",
    "1. **InterprÃ©tation prudente** : Les corrÃ©lations ne sont pas des causalitÃ©s\n",
    "2. **Validation externe nÃ©cessaire** : RÃ©sultats Ã  valider sur d'autres populations\n",
    "3. **ConsidÃ©rations Ã©thiques** : Biais potentiels dans les donnÃ©es\n",
    "\n",
    "## ðŸš€ 7. PROCHAINES Ã‰TAPES - MODÃˆLE MULTI-TÃ‚CHES\n",
    "\n",
    "### Architecture recommandÃ©e\n",
    "1. **Backbone** : EfficientNet ou ResNet prÃ©-entraÃ®nÃ©\n",
    "2. **TÃ¢ches multiples** :\n",
    "   - Classification diagnostique principale\n",
    "   - PrÃ©diction des mÃ©tadonnÃ©es (Ã¢ge, sexe, localisation)\n",
    "   - Segmentation des lÃ©sions (optionnelle)\n",
    "3. **Loss adaptative** : Poids selon l'importance clinique\n",
    "\n",
    "### StratÃ©gie d'entraÃ®nement\n",
    "1. **Transfer learning** : Fine-tuning Ã  partir de modÃ¨les mÃ©dicaux\n",
    "2. **Augmentation** : SpÃ©cifique aux lÃ©sions cutanÃ©es\n",
    "3. **Validation** : Cross-validation stratifiÃ©e\n",
    "4. **MÃ©triques** : AUC-ROC, SensibilitÃ©, SpÃ©cificitÃ©, F1-Score\n",
    "\n",
    "### DÃ©ploiement potentiel\n",
    "1. **Interface** : Web-based avec upload d'images\n",
    "2. **ExplicabilitÃ©** : Cartes d'attention Grad-CAM\n",
    "3. **ConformitÃ©** : Respect des rÃ©gulations mÃ©dicales\n",
    "\n",
    "---\n",
    "\n",
    "*Rapport gÃ©nÃ©rÃ© automatiquement - Analyse avancÃ©e ISIC 2019*\n",
    "*PrÃªt pour la phase de modÃ©lisation multi-tÃ¢ches*\n",
    "\"\"\"\n",
    "\n",
    "# Sauvegarder le rapport\n",
    "advanced_report_path = EDA_DIR / 'reports' / 'ADVANCED_ANALYSIS_REPORT.md'\n",
    "with open(advanced_report_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(advanced_report)\n",
    "\n",
    "print(f\"\\nâœ… RAPPORT AVANCÃ‰ GÃ‰NÃ‰RÃ‰ ET SAUVEGARDÃ‰ :\")\n",
    "print(f\"    {advanced_report_path}\")\n",
    "\n",
    "# Afficher un rÃ©sumÃ©\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"   ðŸ“Š RÃ‰SUMÃ‰ DES PRINCIPALES DÃ‰COUVERTES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nðŸ“ DONNÃ‰ES :\")\n",
    "print(f\"  â€¢ Images totales : {len(df):,} (70.1% du dataset)\")\n",
    "print(f\"  â€¢ Classes diagnostiques : {len(classes)}\")\n",
    "if 'class_stats' in locals() and 'imbalance_ratio' in class_stats:\n",
    "    print(f\"  â€¢ DÃ©sÃ©quilibre max : {class_stats['imbalance_ratio']:.1f}:1\")\n",
    "\n",
    "print(f\"\\nðŸŽ¨ COULEURS :\")\n",
    "if len(dominant_colors) > 0:\n",
    "    print(f\"  â€¢ Palette dominante : RGB({dominant_colors[:, 0].mean():.0f}, {dominant_colors[:, 1].mean():.0f}, {dominant_colors[:, 2].mean():.0f})\")\n",
    "    if 'color_stats' in locals() and 'class_stats' in color_stats and color_stats['class_stats']:\n",
    "        print(f\"  â€¢ Variations diagnostiques : IdentifiÃ©es\")\n",
    "    else:\n",
    "        print(f\"  â€¢ Variations diagnostiques : Non disponibles\")\n",
    "else:\n",
    "    print(f\"  â€¢ Analyse couleur : Non disponible\")\n",
    "\n",
    "print(f\"\\nðŸ”— CORRÃ‰LATIONS :\")\n",
    "if 'significant_correlations' in locals() and significant_correlations:\n",
    "    top_corr = significant_correlations[0]\n",
    "    metadata_name = top_corr[0].replace('site_', '').replace('_', ' ').title()\n",
    "    print(f\"  â€¢ Plus forte corrÃ©lation : {metadata_name} â†” {top_corr[1]} (r={top_corr[2]:.3f})\")\n",
    "else:\n",
    "    print(f\"  â€¢ CorrÃ©lations : Faibles ou non significatives\")\n",
    "\n",
    "print(f\"\\nðŸ“Š PCA :\")\n",
    "if 'pca_stats' in locals():\n",
    "    try:\n",
    "        if ('explained_variance_ratio' in pca_stats and\n",
    "            pca_stats['explained_variance_ratio'] and\n",
    "            len(pca_stats['explained_variance_ratio']) >= 3):\n",
    "            variance_array = np.array(pca_stats['explained_variance_ratio'][:3])\n",
    "            print(f\"  â€¢ Variance expliquÃ©e (3 PC) : {variance_array.sum()*100:.1f}%\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    if 'optimal_clusters_suggested' in pca_stats and pca_stats['optimal_clusters_suggested']:\n",
    "        print(f\"  â€¢ Clusters suggÃ©rÃ©s : {pca_stats['optimal_clusters_suggested']}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ CLUSTERING VISUEL :\")\n",
    "if 'clustering_stats' in locals():\n",
    "    if 'optimal_k_final' in clustering_stats and clustering_stats['optimal_k_final']:\n",
    "        print(f\"  â€¢ Clusters optimaux : {clustering_stats['optimal_k_final']}\")\n",
    "\n",
    "    if 'max_silhouette_score' in clustering_stats and clustering_stats['max_silhouette_score']:\n",
    "        print(f\"  â€¢ Score silhouette : {clustering_stats['max_silhouette_score']:.3f}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(\"  âœ… ANALYSE EXPLORATOIRE AVANCÃ‰E TERMINÃ‰E !\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Sauvegarder toutes les donnÃ©es d'analyse avancÃ©e\n",
    "print(f\"\\nðŸ’¾ Sauvegarde des donnÃ©es d'analyse avancÃ©e...\")\n",
    "\n",
    "advanced_analysis_data = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'dataset_summary': {\n",
    "        'total_analyzed': len(df),\n",
    "        'classes': classes\n",
    "    }\n",
    "}\n",
    "\n",
    "# Ajouter les statistiques de classe si disponibles\n",
    "if 'class_stats' in locals():\n",
    "    if 'coverage_percentage' in class_stats:\n",
    "        advanced_analysis_data['dataset_summary']['coverage_percentage'] = class_stats['coverage_percentage']\n",
    "    if 'imbalance_ratio' in class_stats:\n",
    "        advanced_analysis_data['dataset_summary']['imbalance_ratio'] = class_stats['imbalance_ratio']\n",
    "\n",
    "# Ajouter les donnÃ©es de couleur si disponibles\n",
    "if len(dominant_colors) > 0:\n",
    "    advanced_analysis_data['color_analysis'] = {\n",
    "        'dominant_colors_mean': dominant_colors.mean(axis=0).tolist(),\n",
    "        'dominant_colors_std': dominant_colors.std(axis=0).tolist(),\n",
    "        'samples_analyzed': len(dominant_colors)\n",
    "    }\n",
    "\n",
    "# Ajouter les corrÃ©lations si disponibles\n",
    "if 'significant_correlations' in locals() and significant_correlations:\n",
    "    advanced_analysis_data['correlation_analysis'] = {\n",
    "        'top_correlations': [\n",
    "            {'metadata': m, 'class': c, 'correlation': float(corr)}\n",
    "            for m, c, corr in significant_correlations[:10]\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# Ajouter les statistiques PCA si disponibles\n",
    "if 'pca_stats' in locals():\n",
    "    advanced_analysis_data['pca_analysis'] = pca_stats\n",
    "\n",
    "# Ajouter les statistiques de clustering si disponibles\n",
    "if 'clustering_stats' in locals():\n",
    "    advanced_analysis_data['visual_clustering'] = clustering_stats\n",
    "\n",
    "# Sauvegarder\n",
    "with open(EDA_DIR / 'statistics' / 'advanced_analysis_summary.json', 'w') as f:\n",
    "    json.dump(advanced_analysis_data, f, indent=2)\n",
    "\n",
    "print(f\"âœ… DonnÃ©es d'analyse avancÃ©e sauvegardÃ©es\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(\"    ANALYSE COMPLÃˆTE TERMINÃ‰E AVEC SUCCÃˆS !\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nðŸ“‹ PROCHAINE Ã‰TAPE : PRÃ‰TRAITEMENT POUR LE MODÃˆLE MULTI-TÃ‚CHES\")\n",
    "print(f\"\\n1. âœ… EDA de base terminÃ©e\")\n",
    "print(f\"2. âœ… Analyse avancÃ©e terminÃ©e\")\n",
    "print(f\"3. â³ PrÃªt pour le prÃ©traitement et la modÃ©lisation\")\n",
    "print(f\"\\nðŸ“‚ Tous les rÃ©sultats sont dans : {EDA_DIR}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "eXXNKstC9eg6",
    "outputId": "06cdef36-4709-46a0-d1a0-f1c73918eec7"
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PRÃ‰TRAITEMENT ISIC 2019 â€“ STANDARD MÃ‰DICAL PROFESSIONNEL\n",
    "# Conforme aux exigences du domaine dermatologique et imagerie mÃ©dicale\n",
    "# =============================================================================\n",
    "\n",
    "import os, cv2, numpy as np, pandas as pd, json, hashlib, zipfile\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from google.colab import files\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION MÃ‰DICALE PROFESSIONNELLE\n",
    "# =============================================================================\n",
    "\n",
    "class MedicalPreprocessConfig:\n",
    "    \"\"\"Configuration validÃ©e pour l'imagerie dermatoscopique\"\"\"\n",
    "\n",
    "    # Standards d'imagerie dermatologique\n",
    "    TARGET_SIZE = 384  # RÃ©solution validÃ©e pour analyse dermatologique\n",
    "    INTERPOLATION = cv2.INTER_LANCZOS4  # Meilleure qualitÃ© pour imagerie mÃ©dicale\n",
    "\n",
    "    # Normalisation ImageNet (standard pour transfer learning mÃ©dical)\n",
    "    MEAN = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "    STD = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "\n",
    "    # ParamÃ¨tres de qualitÃ© et validation\n",
    "    MIN_IMAGE_SIZE = 50  # Taille minimale acceptable\n",
    "    MAX_ASPECT_RATIO = 3.0  # Ratio max hauteur/largeur\n",
    "    QUALITY_THRESHOLD = 10  # Seuil variance pour dÃ©tecter images corrompues\n",
    "\n",
    "    # Gestion des classes dÃ©sÃ©quilibrÃ©es (approche conservatrice)\n",
    "    RARE_CLASSES = ['DF', 'VASC', 'AK', 'SCC']\n",
    "    AUGMENTATION_FACTOR = 4  # Facteur rÃ©duit pour Ã©viter l'overfitting\n",
    "\n",
    "    # Chemins (Ã  adapter selon votre environnement)\n",
    "    BASE_DIR = Path('/content/drive/MyDrive/ISIC_2019_Project/data')\n",
    "    IMG_DIR = BASE_DIR / 'ISIC_2019_Training_Input/ISIC_2019_Training_Input'\n",
    "    OUTPUT_ROOT = Path('/content/ISIC2019_Medical_Professional')\n",
    "    VISUALIZATION_DIR = OUTPUT_ROOT / 'visualizations'\n",
    "\n",
    "CONFIG = MedicalPreprocessConfig()\n",
    "\n",
    "# =============================================================================\n",
    "# PIPELINE DE PRÃ‰TRAITEMENT MÃ‰DICAL\n",
    "# =============================================================================\n",
    "\n",
    "class MedicalImagePreprocessor:\n",
    "    \"\"\"\n",
    "    PrÃ©processeur conforme aux standards d'imagerie dermatologique.\n",
    "\n",
    "    Ã‰tapes validÃ©es:\n",
    "    1. Validation qualitÃ© image\n",
    "    2. Removal artefacts (poils) - mÃ©thode Dull Razor\n",
    "    3. Normalisation couleur CLAHE adaptative\n",
    "    4. Segmentation ROI (rÃ©gion d'intÃ©rÃªt)\n",
    "    5. Padding et resize avec conservation aspect ratio\n",
    "    6. Normalisation statistique\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.processing_log = []\n",
    "        self.visualization_samples = []  # Pour stocker exemples avant/aprÃ¨s\n",
    "\n",
    "    def validate_image(self, img, img_name):\n",
    "        \"\"\"Validation qualitÃ© selon standards mÃ©dicaux\"\"\"\n",
    "        if img is None:\n",
    "            self.log_issue(img_name, \"Image non lisible\")\n",
    "            return False\n",
    "\n",
    "        h, w = img.shape[:2]\n",
    "\n",
    "        # VÃ©rifications dimensionnelles\n",
    "        if h < self.config.MIN_IMAGE_SIZE or w < self.config.MIN_IMAGE_SIZE:\n",
    "            self.log_issue(img_name, f\"Dimensions insuffisantes: {w}x{h}\")\n",
    "            return False\n",
    "\n",
    "        aspect_ratio = max(h/w, w/h)\n",
    "        if aspect_ratio > self.config.MAX_ASPECT_RATIO:\n",
    "            self.log_issue(img_name, f\"Aspect ratio anormal: {aspect_ratio:.2f}\")\n",
    "            return False\n",
    "\n",
    "        # DÃ©tection images corrompues/uniformes\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        variance = np.var(gray)\n",
    "        if variance < self.config.QUALITY_THRESHOLD:\n",
    "            self.log_issue(img_name, f\"Variance trop faible: {variance:.2f}\")\n",
    "            return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    def remove_hair_artifacts(self, img):\n",
    "        \"\"\"\n",
    "        Hair removal adaptÃ© - MÃ©thode Dull Razor optimisÃ©e\n",
    "        RÃ©fÃ©rence: Lee et al. \"DullRazor: A Software Approach to Hair Removal\"\n",
    "        \"\"\"\n",
    "        # Conversion espace de travail\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        # DÃ©tection poils via morphologie\n",
    "        kernel_size = max(15, int(min(img.shape[:2]) * 0.02))  # Adaptatif Ã  la taille\n",
    "        if kernel_size % 2 == 0:\n",
    "            kernel_size += 1\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (kernel_size, kernel_size))\n",
    "\n",
    "        # Black-hat transform pour isoler structures sombres fines\n",
    "        blackhat = cv2.morphologyEx(gray, cv2.MORPH_BLACKHAT, kernel)\n",
    "\n",
    "        # Seuillage adaptatif\n",
    "        _, mask = cv2.threshold(blackhat, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "        # Inpainting conservatif\n",
    "        result = cv2.inpaint(img, mask, inpaintRadius=3, flags=cv2.INPAINT_TELEA)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def adaptive_color_normalization(self, img):\n",
    "        \"\"\"\n",
    "        Normalisation couleur adaptative pour compenser variations d'Ã©clairage.\n",
    "        Utilise CLAHE en espace LAB (standard dermatologique)\n",
    "        \"\"\"\n",
    "        # Conversion LAB (perceptuellement uniforme)\n",
    "        lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
    "        l, a, b = cv2.split(lab)\n",
    "\n",
    "        # CLAHE sur canal luminance\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.5, tileGridSize=(8, 8))\n",
    "        l_enhanced = clahe.apply(l)\n",
    "\n",
    "        # Reconstruction\n",
    "        enhanced = cv2.merge([l_enhanced, a, b])\n",
    "        result = cv2.cvtColor(enhanced, cv2.COLOR_LAB2RGB)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def extract_roi_with_padding(self, img):\n",
    "        \"\"\"\n",
    "        Extraction ROI (Region of Interest) avec padding intelligent.\n",
    "        Ã‰limine le fond noir tout en conservant la lÃ©sion complÃ¨te.\n",
    "        \"\"\"\n",
    "        # DÃ©tection du contenu\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        _, binary = cv2.threshold(gray, 15, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        # Nettoyage morphologique\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "        binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
    "        binary = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "        # Extraction bounding box avec marge de sÃ©curitÃ©\n",
    "        coords = cv2.findNonZero(binary)\n",
    "        if coords is not None:\n",
    "            x, y, w, h = cv2.boundingRect(coords)\n",
    "\n",
    "            # Marge de sÃ©curitÃ© (5% de chaque cÃ´tÃ©)\n",
    "            margin_w = int(w * 0.05)\n",
    "            margin_h = int(h * 0.05)\n",
    "\n",
    "            x = max(0, x - margin_w)\n",
    "            y = max(0, y - margin_h)\n",
    "            w = min(img.shape[1] - x, w + 2 * margin_w)\n",
    "            h = min(img.shape[0] - y, h + 2 * margin_h)\n",
    "\n",
    "            img = img[y:y+h, x:x+w]\n",
    "\n",
    "        # Padding carrÃ© avec noir (prÃ©serve contexte mÃ©dical)\n",
    "        h, w = img.shape[:2]\n",
    "        max_dim = max(h, w)\n",
    "\n",
    "        # Canvas noir\n",
    "        padded = np.zeros((max_dim, max_dim, 3), dtype=img.dtype)\n",
    "\n",
    "        # Centrage\n",
    "        y_offset = (max_dim - h) // 2\n",
    "        x_offset = (max_dim - w) // 2\n",
    "        padded[y_offset:y_offset+h, x_offset:x_offset+w] = img\n",
    "\n",
    "        return padded\n",
    "\n",
    "    def resize_preserve_quality(self, img, target_size):\n",
    "        \"\"\"Resize avec interpolation haute qualitÃ©\"\"\"\n",
    "        return cv2.resize(img, (target_size, target_size),\n",
    "                         interpolation=self.config.INTERPOLATION)\n",
    "\n",
    "    def normalize_intensity(self, img):\n",
    "        \"\"\"Normalisation statistique finale\"\"\"\n",
    "        img_float = img.astype(np.float32) / 255.0\n",
    "        normalized = (img_float - self.config.MEAN) / self.config.STD\n",
    "        return normalized\n",
    "\n",
    "    def process_image(self, img_path, img_name, save_steps=False):\n",
    "        \"\"\"Pipeline complet de prÃ©traitement avec option de sauvegarde Ã©tapes\"\"\"\n",
    "        try:\n",
    "            # Lecture\n",
    "            img_original = cv2.imread(str(img_path))\n",
    "            if img_original is None:\n",
    "                return None, None\n",
    "            img_original = cv2.cvtColor(img_original, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Validation\n",
    "            if not self.validate_image(img_original, img_name):\n",
    "                return None, None\n",
    "\n",
    "            # Stockage des Ã©tapes pour visualisation\n",
    "            steps = {} if save_steps else None\n",
    "            if save_steps:\n",
    "                steps['original'] = img_original.copy()\n",
    "\n",
    "            # Pipeline avec sauvegarde Ã©tapes\n",
    "            img = img_original.copy()\n",
    "\n",
    "            # Ã‰tape 1: Hair removal\n",
    "            img = self.remove_hair_artifacts(img)\n",
    "            if save_steps:\n",
    "                steps['hair_removed'] = img.copy()\n",
    "\n",
    "            # Ã‰tape 2: Normalisation couleur\n",
    "            img = self.adaptive_color_normalization(img)\n",
    "            if save_steps:\n",
    "                steps['color_normalized'] = img.copy()\n",
    "\n",
    "            # Ã‰tape 3: ROI extraction\n",
    "            img = self.extract_roi_with_padding(img)\n",
    "            if save_steps:\n",
    "                steps['roi_extracted'] = img.copy()\n",
    "\n",
    "            # Ã‰tape 4: Resize\n",
    "            img = self.resize_preserve_quality(img, self.config.TARGET_SIZE)\n",
    "            if save_steps:\n",
    "                steps['resized'] = img.copy()\n",
    "\n",
    "            # Ã‰tape 5: Normalisation finale\n",
    "            img = self.normalize_intensity(img)\n",
    "            if save_steps:\n",
    "                steps['normalized'] = img\n",
    "\n",
    "            return img, steps\n",
    "\n",
    "        except Exception as e:\n",
    "            self.log_issue(img_name, f\"Erreur traitement: {str(e)}\")\n",
    "            return None, None\n",
    "\n",
    "    def log_issue(self, img_name, issue):\n",
    "        \"\"\"Journalisation des problÃ¨mes pour traÃ§abilitÃ©\"\"\"\n",
    "        self.processing_log.append({\n",
    "            'image': img_name,\n",
    "            'issue': issue,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        })\n",
    "\n",
    "    def save_visualization_sample(self, img_name, steps, label):\n",
    "        \"\"\"Sauvegarde un Ã©chantillon pour visualisation\"\"\"\n",
    "        self.visualization_samples.append({\n",
    "            'image_name': img_name,\n",
    "            'steps': steps,\n",
    "            'label': label\n",
    "        })\n",
    "\n",
    "# =============================================================================\n",
    "# FONCTIONS DE VISUALISATION\n",
    "# =============================================================================\n",
    "\n",
    "def create_processing_visualization(sample, output_path):\n",
    "    \"\"\"CrÃ©e une visualisation avant/aprÃ¨s du pipeline\"\"\"\n",
    "    steps = sample['steps']\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle(f\"Pipeline de PrÃ©traitement: {sample['image_name']} (Classe: {sample['label']})\",\n",
    "                 fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Ordre des Ã©tapes Ã  afficher\n",
    "    step_order = [\n",
    "        ('original', 'Image Originale'),\n",
    "        ('hair_removed', 'Removal Cheveux'),\n",
    "        ('color_normalized', 'Normalisation Couleur (CLAHE)'),\n",
    "        ('roi_extracted', 'Extraction ROI + Padding'),\n",
    "        ('resized', 'Resize 384x384'),\n",
    "        ('normalized', 'Normalisation Finale')\n",
    "    ]\n",
    "\n",
    "    for idx, (step_key, step_title) in enumerate(step_order):\n",
    "        row = idx // 3\n",
    "        col = idx % 3\n",
    "        ax = axes[row, col]\n",
    "\n",
    "        if step_key in steps:\n",
    "            img = steps[step_key]\n",
    "\n",
    "            # DÃ©normalisation si Ã©tape finale\n",
    "            if step_key == 'normalized':\n",
    "                img_display = img * CONFIG.STD + CONFIG.MEAN\n",
    "                img_display = np.clip(img_display * 255, 0, 255).astype(np.uint8)\n",
    "            else:\n",
    "                img_display = img\n",
    "\n",
    "            ax.imshow(img_display)\n",
    "            ax.set_title(step_title, fontsize=12, fontweight='bold')\n",
    "            ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def create_distribution_plots(train_df, val_df, test_df, output_dir):\n",
    "    \"\"\"CrÃ©e les graphiques de distribution des classes\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Distribution des Classes - Dataset ISIC 2019', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Palette de couleurs\n",
    "    colors = sns.color_palette(\"Set2\", 8)\n",
    "\n",
    "    # 1. Distribution globale\n",
    "    ax = axes[0, 0]\n",
    "    all_data = pd.concat([train_df, val_df, test_df])\n",
    "    class_counts = all_data['label'].value_counts().sort_index()\n",
    "    ax.bar(range(len(class_counts)), class_counts.values, color=colors)\n",
    "    ax.set_xticks(range(len(class_counts)))\n",
    "    ax.set_xticklabels(class_counts.index, rotation=45)\n",
    "    ax.set_title('Distribution Globale', fontweight='bold')\n",
    "    ax.set_ylabel('Nombre d\\'images')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    # 2. Distribution par split\n",
    "    ax = axes[0, 1]\n",
    "    split_data = {\n",
    "        'Train': train_df['label'].value_counts().sort_index(),\n",
    "        'Val': val_df['label'].value_counts().sort_index(),\n",
    "        'Test': test_df['label'].value_counts().sort_index()\n",
    "    }\n",
    "    x = np.arange(len(class_counts))\n",
    "    width = 0.25\n",
    "    for idx, (split_name, counts) in enumerate(split_data.items()):\n",
    "        ax.bar(x + idx*width, counts.values, width, label=split_name, alpha=0.8)\n",
    "    ax.set_xticks(x + width)\n",
    "    ax.set_xticklabels(class_counts.index, rotation=45)\n",
    "    ax.set_title('Distribution par Split', fontweight='bold')\n",
    "    ax.set_ylabel('Nombre d\\'images')\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "    # 3. Proportions en camembert - Train\n",
    "    ax = axes[1, 0]\n",
    "    train_counts = train_df['label'].value_counts().sort_index()\n",
    "    ax.pie(train_counts.values, labels=train_counts.index, autopct='%1.1f%%',\n",
    "           colors=colors, startangle=90)\n",
    "    ax.set_title('Proportions Train', fontweight='bold')\n",
    "\n",
    "    # 4. Tableau rÃ©capitulatif\n",
    "    ax = axes[1, 1]\n",
    "    ax.axis('off')\n",
    "\n",
    "    summary_data = []\n",
    "    for label in class_counts.index:\n",
    "        train_count = train_df[train_df['label'] == label].shape[0]\n",
    "        val_count = val_df[val_df['label'] == label].shape[0]\n",
    "        test_count = test_df[test_df['label'] == label].shape[0]\n",
    "        total = train_count + val_count + test_count\n",
    "        summary_data.append([label, train_count, val_count, test_count, total])\n",
    "\n",
    "    table = ax.table(cellText=summary_data,\n",
    "                     colLabels=['Classe', 'Train', 'Val', 'Test', 'Total'],\n",
    "                     cellLoc='center',\n",
    "                     loc='center',\n",
    "                     bbox=[0, 0, 1, 1])\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1, 2)\n",
    "\n",
    "    # Style du header\n",
    "    for i in range(5):\n",
    "        table[(0, i)].set_facecolor('#4CAF50')\n",
    "        table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "\n",
    "    ax.set_title('Tableau RÃ©capitulatif', fontweight='bold', pad=20)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'class_distribution.png', dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def create_quality_report_visualization(quality_report, processing_log, output_dir):\n",
    "    \"\"\"CrÃ©e une visualisation du rapport qualitÃ©\"\"\"\n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "    gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)\n",
    "\n",
    "    fig.suptitle('Rapport QualitÃ© du PrÃ©traitement', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # 1. Statistiques gÃ©nÃ©rales\n",
    "    ax1 = fig.add_subplot(gs[0, :])\n",
    "    ax1.axis('off')\n",
    "\n",
    "    stats_text = f\"\"\"\n",
    "    ðŸ“Š STATISTIQUES GÃ‰NÃ‰RALES\n",
    "\n",
    "    Date de traitement: {quality_report['processing_date'][:10]}\n",
    "    Images totales: {quality_report['total_images']}\n",
    "\n",
    "    ðŸ“ RÃ‰PARTITION\n",
    "    Train: {quality_report['train_images']} images ({quality_report['train_images']/quality_report['total_images']*100:.1f}%)\n",
    "    Validation: {quality_report['val_images']} images ({quality_report['val_images']/quality_report['total_images']*100:.1f}%)\n",
    "    Test: {quality_report['test_images']} images ({quality_report['test_images']/quality_report['total_images']*100:.1f}%)\n",
    "\n",
    "    âš™ï¸ PARAMÃˆTRES\n",
    "    Taille cible: {quality_report['target_size']}x{quality_report['target_size']} pixels\n",
    "    Facteur augmentation: x{quality_report['augmentation_factor']} (classes rares uniquement)\n",
    "    Classes rares: {', '.join(quality_report['rare_classes'])}\n",
    "    \"\"\"\n",
    "\n",
    "    ax1.text(0.1, 0.5, stats_text, fontsize=12, verticalalignment='center',\n",
    "             fontfamily='monospace', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
    "\n",
    "    # 2. Issues dÃ©tectÃ©es\n",
    "    ax2 = fig.add_subplot(gs[1, :])\n",
    "\n",
    "    if len(processing_log) > 0:\n",
    "        issue_types = {}\n",
    "        for log in processing_log:\n",
    "            issue = log['issue'].split(':')[0]\n",
    "            issue_types[issue] = issue_types.get(issue, 0) + 1\n",
    "\n",
    "        issues = list(issue_types.keys())\n",
    "        counts = list(issue_types.values())\n",
    "\n",
    "        colors_issues = sns.color_palette(\"Reds\", len(issues))\n",
    "        ax2.barh(issues, counts, color=colors_issues)\n",
    "        ax2.set_xlabel('Nombre d\\'occurrences')\n",
    "        ax2.set_title(f'âš ï¸ ProblÃ¨mes DÃ©tectÃ©s ({len(processing_log)} total)', fontweight='bold')\n",
    "        ax2.grid(axis='x', alpha=0.3)\n",
    "    else:\n",
    "        ax2.text(0.5, 0.5, 'âœ… Aucun problÃ¨me dÃ©tectÃ©\\nToutes les images traitÃ©es avec succÃ¨s',\n",
    "                ha='center', va='center', fontsize=14, fontweight='bold',\n",
    "                bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))\n",
    "        ax2.axis('off')\n",
    "\n",
    "    # 3. Pipeline de traitement\n",
    "    ax3 = fig.add_subplot(gs[2, :])\n",
    "    ax3.axis('off')\n",
    "\n",
    "    pipeline_text = \"\"\"\n",
    "    ðŸ”¬ PIPELINE DE PRÃ‰TRAITEMENT MÃ‰DICAL\n",
    "\n",
    "    âœ“ Validation qualitÃ© (dimensions, aspect ratio, variance)\n",
    "    âœ“ Removal artefacts cheveux (MÃ©thode Dull Razor)\n",
    "    âœ“ Normalisation couleur adaptative (CLAHE en LAB)\n",
    "    âœ“ Extraction ROI avec marges de sÃ©curitÃ© (5%)\n",
    "    âœ“ Padding intelligent et centrage\n",
    "    âœ“ Resize haute qualitÃ© (LANCZOS4)\n",
    "    âœ“ Normalisation statistique (ImageNet: Î¼={}, Ïƒ={})\n",
    "    âœ“ Augmentation conservatrice (gÃ©omÃ©trique uniquement)\n",
    "    \"\"\".format(\n",
    "        [f\"{m:.3f}\" for m in quality_report['normalization']['mean']],\n",
    "        [f\"{s:.3f}\" for s in quality_report['normalization']['std']]\n",
    "    )\n",
    "\n",
    "    ax3.text(0.1, 0.5, pipeline_text, fontsize=11, verticalalignment='center',\n",
    "             fontfamily='monospace', bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.3))\n",
    "\n",
    "    plt.savefig(output_dir / 'quality_report.png', dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# =============================================================================\n",
    "# AUGMENTATION MÃ‰DICALE CONSERVATRICE\n",
    "# =============================================================================\n",
    "\n",
    "def get_medical_augmentation_pipeline():\n",
    "    \"\"\"\n",
    "    Augmentations validÃ©es pour imagerie dermatologique.\n",
    "    Uniquement transformations gÃ©omÃ©triques prÃ©servant les caractÃ©ristiques cliniques.\n",
    "    \"\"\"\n",
    "    return A.Compose([\n",
    "        # Transformations gÃ©omÃ©triques (cliniquement neutres)\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.ShiftScaleRotate(\n",
    "            shift_limit=0.05,\n",
    "            scale_limit=0.1,\n",
    "            rotate_limit=15,\n",
    "            border_mode=cv2.BORDER_CONSTANT,\n",
    "            value=0,\n",
    "            p=0.5\n",
    "        ),\n",
    "\n",
    "        # Variations photomÃ©triques lÃ©gÃ¨res (simulent conditions acquisition)\n",
    "        A.RandomBrightnessContrast(\n",
    "            brightness_limit=0.1,\n",
    "            contrast_limit=0.1,\n",
    "            p=0.3\n",
    "        ),\n",
    "\n",
    "        # Pas de dÃ©formations Ã©lastiques ni de modifications de couleur\n",
    "        # qui altÃ©reraient les caractÃ©ristiques diagnostiques\n",
    "    ], p=0.8)\n",
    "\n",
    "# =============================================================================\n",
    "# STRATIFICATION ET SPLIT PROFESSIONNEL\n",
    "# =============================================================================\n",
    "\n",
    "def create_stratified_splits(df, config, n_folds=5, test_size=0.15):\n",
    "    \"\"\"\n",
    "    Split stratifiÃ© avec validation croisÃ©e optionnelle.\n",
    "    Garantit distribution Ã©quilibrÃ©e des classes dans tous les sets.\n",
    "    \"\"\"\n",
    "    print(\"\\nðŸ”¬ CrÃ©ation des splits stratifiÃ©s...\")\n",
    "\n",
    "    # Split initial test\n",
    "    test_indices = []\n",
    "    train_val_indices = []\n",
    "\n",
    "    for label in df['label'].unique():\n",
    "        label_indices = df[df['label'] == label].index.tolist()\n",
    "        n_test = max(1, int(len(label_indices) * test_size))\n",
    "\n",
    "        np.random.seed(42)\n",
    "        test_idx = np.random.choice(label_indices, size=n_test, replace=False)\n",
    "        train_val_idx = [i for i in label_indices if i not in test_idx]\n",
    "\n",
    "        test_indices.extend(test_idx)\n",
    "        train_val_indices.extend(train_val_idx)\n",
    "\n",
    "    test_df = df.loc[test_indices].reset_index(drop=True)\n",
    "    train_val_df = df.loc[train_val_indices].reset_index(drop=True)\n",
    "\n",
    "    # Split train/val stratifiÃ©\n",
    "    val_size = 0.15 / (1 - test_size)  # ~15% du total\n",
    "    val_indices = []\n",
    "    train_indices = []\n",
    "\n",
    "    for label in train_val_df['label'].unique():\n",
    "        label_indices = train_val_df[train_val_df['label'] == label].index.tolist()\n",
    "        n_val = max(1, int(len(label_indices) * val_size))\n",
    "\n",
    "        np.random.seed(42)\n",
    "        val_idx = np.random.choice(label_indices, size=n_val, replace=False)\n",
    "        train_idx = [i for i in label_indices if i not in val_idx]\n",
    "\n",
    "        val_indices.extend(val_idx)\n",
    "        train_indices.extend(train_idx)\n",
    "\n",
    "    train_df = train_val_df.loc[train_indices].reset_index(drop=True)\n",
    "    val_df = train_val_df.loc[val_indices].reset_index(drop=True)\n",
    "\n",
    "    # Statistiques\n",
    "    print(f\"\\nðŸ“Š Distribution des donnÃ©es:\")\n",
    "    print(f\"   Train: {len(train_df)} images ({len(train_df)/len(df)*100:.1f}%)\")\n",
    "    print(f\"   Val:   {len(val_df)} images ({len(val_df)/len(df)*100:.1f}%)\")\n",
    "    print(f\"   Test:  {len(test_df)} images ({len(test_df)/len(df)*100:.1f}%)\")\n",
    "\n",
    "    # VÃ©rification distribution par classe\n",
    "    print(f\"\\nðŸ“‹ Distribution par classe:\")\n",
    "    for split_name, split_df in [('Train', train_df), ('Val', val_df), ('Test', test_df)]:\n",
    "        dist = split_df['label'].value_counts().sort_index()\n",
    "        print(f\"\\n{split_name}:\")\n",
    "        for label, count in dist.items():\n",
    "            print(f\"   {label}: {count:4d} ({count/len(split_df)*100:5.1f}%)\")\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "# =============================================================================\n",
    "# PIPELINE PRINCIPAL\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Pipeline principal de prÃ©traitement mÃ©dical\"\"\"\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\"PRÃ‰TRAITEMENT ISIC 2019 - STANDARD MÃ‰DICAL PROFESSIONNEL\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nâ° DÃ©but: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "    # CrÃ©ation structure de sortie\n",
    "    output_dirs = {\n",
    "        'images': CONFIG.OUTPUT_ROOT / 'images',\n",
    "        'train': CONFIG.OUTPUT_ROOT / 'images' / 'train',\n",
    "        'val': CONFIG.OUTPUT_ROOT / 'images' / 'val',\n",
    "        'test': CONFIG.OUTPUT_ROOT / 'images' / 'test',\n",
    "        'logs': CONFIG.OUTPUT_ROOT / 'logs',\n",
    "        'metadata': CONFIG.OUTPUT_ROOT / 'metadata',\n",
    "        'visualizations': CONFIG.VISUALIZATION_DIR\n",
    "    }\n",
    "\n",
    "    for dir_path in output_dirs.values():\n",
    "        dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Chargement et fusion donnÃ©es\n",
    "    print(\"\\nðŸ“¥ Chargement des mÃ©tadonnÃ©es...\")\n",
    "    df = pd.read_csv(CONFIG.BASE_DIR / 'ISIC_2019_Training_GroundTruth.csv')\n",
    "    meta = pd.read_csv(CONFIG.BASE_DIR / 'ISIC_2019_Training_Metadata.csv')\n",
    "    df = pd.merge(df, meta, on='image', how='inner')\n",
    "\n",
    "    # Validation chemins\n",
    "    df['path'] = df['image'].apply(lambda x: CONFIG.IMG_DIR / f\"{x}.jpg\")\n",
    "    df = df[df['path'].apply(lambda p: p.exists())].reset_index(drop=True)\n",
    "\n",
    "    # Extraction labels\n",
    "    label_cols = ['MEL', 'NV', 'BCC', 'AK', 'BKL', 'DF', 'VASC', 'SCC']\n",
    "    df['label'] = df[label_cols].idxmax(axis=1)\n",
    "\n",
    "    print(f\"âœ… {len(df)} images valides chargÃ©es\")\n",
    "    print(f\"\\nðŸ¥ Distribution initiale des classes:\")\n",
    "    for label, count in df['label'].value_counts().sort_index().items():\n",
    "        print(f\"   {label}: {count:5d} ({count/len(df)*100:.1f}%)\")\n",
    "\n",
    "    # CrÃ©ation splits stratifiÃ©s\n",
    "    train_df, val_df, test_df = create_stratified_splits(df, CONFIG)\n",
    "\n",
    "    # Initialisation prÃ©processeur\n",
    "    preprocessor = MedicalImagePreprocessor(CONFIG)\n",
    "    augmentor = get_medical_augmentation_pipeline()\n",
    "\n",
    "    # SÃ©lection d'Ã©chantillons pour visualisation (2 par classe)\n",
    "    samples_to_visualize = []\n",
    "    for label in df['label'].unique():\n",
    "        label_samples = train_df[train_df['label'] == label].head(2)\n",
    "        samples_to_visualize.extend(label_samples.index.tolist())\n",
    "\n",
    "    print(f\"\\nðŸŽ¨ {len(samples_to_visualize)} Ã©chantillons sÃ©lectionnÃ©s pour visualisation\")\n",
    "\n",
    "    # Traitement par split\n",
    "    for split_name, split_df in [('train', train_df), ('val', val_df), ('test', test_df)]:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"ðŸ”¬ Traitement {split_name.upper()} ({len(split_df)} images)\")\n",
    "        print(f\"{'='*80}\")\n",
    "\n",
    "        processed_count = 0\n",
    "        augmented_count = 0\n",
    "\n",
    "        for idx, row in tqdm(split_df.iterrows(), total=len(split_df),\n",
    "                            desc=f\"Processing {split_name}\"):\n",
    "\n",
    "            # VÃ©rifier si on doit sauvegarder les Ã©tapes pour visualisation\n",
    "            save_steps = (split_name == 'train' and idx in samples_to_visualize)\n",
    "\n",
    "            # Image originale\n",
    "            img_processed, steps = preprocessor.process_image(\n",
    "                row['path'], row['image'], save_steps=save_steps\n",
    "            )\n",
    "\n",
    "            if img_processed is not None:\n",
    "                output_path = output_dirs[split_name] / f\"{row['image']}.npy\"\n",
    "                np.save(output_path, img_processed)\n",
    "                processed_count += 1\n",
    "\n",
    "                # Sauvegarde Ã©chantillon pour visualisation\n",
    "                if save_steps and steps:\n",
    "                    preprocessor.save_visualization_sample(row['image'], steps, row['label'])\n",
    "\n",
    "                # Augmentation uniquement classes rares (train only)\n",
    "                if split_name == 'train' and row['label'] in CONFIG.RARE_CLASSES:\n",
    "                    # DÃ©normalisation pour augmentation\n",
    "                    img_denorm = img_processed * CONFIG.STD + CONFIG.MEAN\n",
    "                    img_uint8 = np.clip(img_denorm * 255, 0, 255).astype(np.uint8)\n",
    "\n",
    "                    for aug_idx in range(CONFIG.AUGMENTATION_FACTOR):\n",
    "                        aug_result = augmentor(image=img_uint8)\n",
    "                        img_aug = aug_result['image']\n",
    "\n",
    "                        # Re-normalisation\n",
    "                        img_aug_norm = (img_aug.astype(np.float32) / 255.0 - CONFIG.MEAN) / CONFIG.STD\n",
    "\n",
    "                        aug_path = output_dirs[split_name] / f\"{row['image']}_aug{aug_idx}.npy\"\n",
    "                        np.save(aug_path, img_aug_norm)\n",
    "                        augmented_count += 1\n",
    "\n",
    "        print(f\"\\nâœ… {split_name.upper()} terminÃ©:\")\n",
    "        print(f\"   - Images traitÃ©es: {processed_count}/{len(split_df)}\")\n",
    "        if augmented_count > 0:\n",
    "            print(f\"   - Images augmentÃ©es: {augmented_count}\")\n",
    "\n",
    "    # Sauvegarde mÃ©tadonnÃ©es\n",
    "    print(\"\\nðŸ’¾ Sauvegarde des mÃ©tadonnÃ©es...\")\n",
    "    train_df.to_csv(output_dirs['metadata'] / 'train_metadata.csv', index=False)\n",
    "    val_df.to_csv(output_dirs['metadata'] / 'val_metadata.csv', index=False)\n",
    "    test_df.to_csv(output_dirs['metadata'] / 'test_metadata.csv', index=False)\n",
    "\n",
    "    # Sauvegarde logs de traitement\n",
    "    if preprocessor.processing_log:\n",
    "        log_df = pd.DataFrame(preprocessor.processing_log)\n",
    "        log_df.to_csv(output_dirs['logs'] / 'processing_issues.csv', index=False)\n",
    "        print(f\"âš ï¸  {len(preprocessor.processing_log)} problÃ¨mes dÃ©tectÃ©s (voir logs)\")\n",
    "\n",
    "    # GÃ©nÃ©ration rapport qualitÃ©\n",
    "    quality_report = {\n",
    "        'processing_date': datetime.now().isoformat(),\n",
    "        'total_images': len(df),\n",
    "        'train_images': len(train_df),\n",
    "        'val_images': len(val_df),\n",
    "        'test_images': len(test_df),\n",
    "        'target_size': CONFIG.TARGET_SIZE,\n",
    "        'normalization': {'mean': CONFIG.MEAN.tolist(), 'std': CONFIG.STD.tolist()},\n",
    "        'augmentation_factor': CONFIG.AUGMENTATION_FACTOR,\n",
    "        'rare_classes': CONFIG.RARE_CLASSES,\n",
    "        'issues_detected': len(preprocessor.processing_log)\n",
    "    }\n",
    "\n",
    "    with open(output_dirs['metadata'] / 'quality_report.json', 'w') as f:\n",
    "        json.dump(quality_report, f, indent=2)\n",
    "\n",
    "    # GÃ‰NÃ‰RATION DES VISUALISATIONS\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸŽ¨ GÃ‰NÃ‰RATION DES VISUALISATIONS\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # 1. Visualisations du pipeline (Ã©chantillons)\n",
    "    print(\"\\nðŸ“¸ CrÃ©ation des visualisations pipeline...\")\n",
    "    for idx, sample in enumerate(tqdm(preprocessor.visualization_samples, desc=\"Pipeline viz\")):\n",
    "        output_file = output_dirs['visualizations'] / f'pipeline_{idx+1}_{sample[\"label\"]}.png'\n",
    "        create_processing_visualization(sample, output_file)\n",
    "\n",
    "    print(f\"âœ… {len(preprocessor.visualization_samples)} visualisations pipeline crÃ©Ã©es\")\n",
    "\n",
    "    # 2. Graphiques de distribution\n",
    "    print(\"\\nðŸ“Š CrÃ©ation des graphiques de distribution...\")\n",
    "    create_distribution_plots(train_df, val_df, test_df, output_dirs['visualizations'])\n",
    "    print(\"âœ… Graphiques de distribution crÃ©Ã©s\")\n",
    "\n",
    "    # 3. Rapport qualitÃ© visuel\n",
    "    print(\"\\nðŸ“‹ CrÃ©ation du rapport qualitÃ© visuel...\")\n",
    "    create_quality_report_visualization(\n",
    "        quality_report,\n",
    "        preprocessor.processing_log,\n",
    "        output_dirs['visualizations']\n",
    "    )\n",
    "    print(\"âœ… Rapport qualitÃ© visuel crÃ©Ã©\")\n",
    "\n",
    "    # CrÃ©ation d'un README\n",
    "    readme_content = f\"\"\"# ISIC 2019 - Dataset PrÃ©traitÃ© (Standard MÃ©dical)\n",
    "\n",
    "## ðŸ“Š Informations GÃ©nÃ©rales\n",
    "\n",
    "- **Date de traitement**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "- **Images totales**: {len(df)}\n",
    "- **Train**: {len(train_df)} images\n",
    "- **Validation**: {len(val_df)} images\n",
    "- **Test**: {len(test_df)} images\n",
    "\n",
    "## ðŸ”¬ Pipeline de PrÃ©traitement\n",
    "\n",
    "1. âœ… **Validation qualitÃ©** (dimensions, aspect ratio, variance)\n",
    "2. âœ… **Removal artefacts cheveux** (MÃ©thode Dull Razor)\n",
    "3. âœ… **Normalisation couleur** (CLAHE adaptatif en LAB)\n",
    "4. âœ… **Extraction ROI** avec marges de sÃ©curitÃ© (5%)\n",
    "5. âœ… **Padding intelligent** et centrage\n",
    "6. âœ… **Resize haute qualitÃ©** (LANCZOS4 â†’ {CONFIG.TARGET_SIZE}x{CONFIG.TARGET_SIZE})\n",
    "7. âœ… **Normalisation statistique** (ImageNet mean/std)\n",
    "8. âœ… **Augmentation conservatrice** (x{CONFIG.AUGMENTATION_FACTOR}, classes rares uniquement)\n",
    "\n",
    "## ðŸ“ Structure du Dataset\n",
    "\n",
    "```\n",
    "ISIC2019_Medical_Professional/\n",
    "â”œâ”€â”€ images/\n",
    "â”‚   â”œâ”€â”€ train/          # Images train (.npy format)\n",
    "â”‚   â”œâ”€â”€ val/            # Images validation (.npy format)\n",
    "â”‚   â””â”€â”€ test/           # Images test (.npy format)\n",
    "â”œâ”€â”€ metadata/\n",
    "â”‚   â”œâ”€â”€ train_metadata.csv\n",
    "â”‚   â”œâ”€â”€ val_metadata.csv\n",
    "â”‚   â”œâ”€â”€ test_metadata.csv\n",
    "â”‚   â””â”€â”€ quality_report.json\n",
    "â”œâ”€â”€ visualizations/\n",
    "â”‚   â”œâ”€â”€ pipeline_*.png          # Visualisations avant/aprÃ¨s\n",
    "â”‚   â”œâ”€â”€ class_distribution.png  # Distribution des classes\n",
    "â”‚   â””â”€â”€ quality_report.png      # Rapport qualitÃ©\n",
    "â”œâ”€â”€ logs/\n",
    "â”‚   â””â”€â”€ processing_issues.csv   # ProblÃ¨mes dÃ©tectÃ©s (si applicable)\n",
    "â””â”€â”€ README.md\n",
    "```\n",
    "\n",
    "## ðŸ¥ Classes (8 lÃ©sions dermatologiques)\n",
    "\n",
    "- **MEL**: Melanoma\n",
    "- **NV**: Melanocytic nevus\n",
    "- **BCC**: Basal cell carcinoma\n",
    "- **AK**: Actinic keratosis (rare - augmentÃ©e)\n",
    "- **BKL**: Benign keratosis\n",
    "- **DF**: Dermatofibroma (rare - augmentÃ©e)\n",
    "- **VASC**: Vascular lesion (rare - augmentÃ©e)\n",
    "- **SCC**: Squamous cell carcinoma (rare - augmentÃ©e)\n",
    "\n",
    "## ðŸ’¾ Utilisation\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Charger une image\n",
    "img = np.load('images/train/ISIC_0000001.npy')\n",
    "\n",
    "# Format: (384, 384, 3), dtype: float32\n",
    "# DÃ©jÃ  normalisÃ© avec mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "```\n",
    "\n",
    "## âš™ï¸ ParamÃ¨tres Techniques\n",
    "\n",
    "- **Taille cible**: {CONFIG.TARGET_SIZE}x{CONFIG.TARGET_SIZE} pixels\n",
    "- **Interpolation**: LANCZOS4 (haute qualitÃ©)\n",
    "- **Normalisation**: ImageNet (mean/std)\n",
    "- **Format**: NumPy .npy (float32)\n",
    "- **Augmentation**: GÃ©omÃ©trique uniquement (prÃ©serve caractÃ©ristiques cliniques)\n",
    "\n",
    "## ðŸ“ˆ Visualisations Incluses\n",
    "\n",
    "Consultez le dossier `visualizations/` pour:\n",
    "- Exemples avant/aprÃ¨s du pipeline complet\n",
    "- Distribution des classes par split\n",
    "- Rapport qualitÃ© dÃ©taillÃ©\n",
    "\n",
    "## âœ… Validation QualitÃ©\n",
    "\n",
    "- {len(preprocessor.processing_log)} problÃ¨me(s) dÃ©tectÃ©(s)\n",
    "- Voir `logs/processing_issues.csv` pour dÃ©tails\n",
    "\n",
    "---\n",
    "**Standard mÃ©dical professionnel | Conforme imagerie dermatoscopique**\n",
    "\"\"\"\n",
    "\n",
    "    with open(CONFIG.OUTPUT_ROOT / 'README.md', 'w', encoding='utf-8') as f:\n",
    "        f.write(readme_content)\n",
    "\n",
    "    # Compression et tÃ©lÃ©chargement vers machine locale\n",
    "    print(\"\\nðŸ“¦ Compression du dataset...\")\n",
    "    zip_filename = 'ISIC2019_Medical_Professional.zip'\n",
    "    zip_path_local = f'/content/{zip_filename}'\n",
    "\n",
    "    # Compression\n",
    "    os.system(f'cd /content && zip -r -q {zip_path_local} {CONFIG.OUTPUT_ROOT.name}')\n",
    "\n",
    "    # TÃ©lÃ©chargement vers votre machine locale\n",
    "    print(\"\\nðŸ“¥ TÃ©lÃ©chargement vers votre machine locale...\")\n",
    "    print(\"â³ Le tÃ©lÃ©chargement va dÃ©marrer automatiquement...\")\n",
    "    files.download(zip_path_local)\n",
    "\n",
    "    print(f\"\\nâœ… PrÃ©traitement terminÃ© avec succÃ¨s!\")\n",
    "    print(f\"â° Fin: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"\\nðŸ“Š RÃ©sumÃ©:\")\n",
    "    print(f\"   - Dataset prÃªt pour entraÃ®nement professionnel\")\n",
    "    print(f\"   - QualitÃ© mÃ©dicale garantie\")\n",
    "    print(f\"   - TraÃ§abilitÃ© complÃ¨te assurÃ©e\")\n",
    "    print(f\"\\nðŸ’¾ Fichier ZIP:\")\n",
    "    print(f\"   - âœ… TÃ©lÃ©chargÃ© sur votre machine locale\")\n",
    "    print(f\"   - ðŸ“ VÃ©rifiez votre dossier 'TÃ©lÃ©chargements'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "S2xtx9qCV57B",
    "outputId": "7a6e7209-a417-4457-b1bd-1516c56cde7e"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "# Chemin du fichier zip dans Colab\n",
    "source = '/content/ISIC2019_Medical_Professional.zip'\n",
    "\n",
    "# VÃ©rifier que le fichier existe\n",
    "if os.path.exists(source):\n",
    "    print(\"TÃ©lÃ©chargement en cours...\")\n",
    "    files.download(source)\n",
    "    print(\"âœ… TÃ©lÃ©chargement lancÃ© ! VÃ©rifiez le dossier 'TÃ©lÃ©chargements' de votre ordinateur.\")\n",
    "else:\n",
    "    print(\"âŒ Fichier introuvable dans Colab :\", source)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
